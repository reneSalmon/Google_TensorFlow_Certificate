{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow - Exam practise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "9LEN1H22f2zs",
        "YShLWZuA0RIv",
        "LRoqLzxB9gU9",
        "1AnDmEu3gMew"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1: TF.Basics\n"
      ],
      "metadata": {
        "id": "9LEN1H22f2zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Quickstart for Beginners"
      ],
      "metadata": {
        "id": "ueEq3n3_gd1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a prebuild dataset\n",
        "2. Build neural network that classifies images\n",
        "3. Train neural network\n",
        "4. Evaluat accuracy of model"
      ],
      "metadata": {
        "id": "AjGO40BLhEam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfCGSB4ahD98",
        "outputId": "dc1df20b-364d-4aa4-9294-ec4fcd047964"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTgTW33HfwCM",
        "outputId": "a09e811a-13a7-41a2-8c5a-e2a104af0a1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 1 Load dataset\n",
        "minst = tf.keras.datasets.mnist #load dataset of 70.000 handwriting digits for image classification\n",
        "(x_train, y_train), (x_test, y_test) = minst.load_data() #split dataset x = images, y = image labels\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 #normalize dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])\n",
        "print(y_train[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7rww5zcfeHpb",
        "outputId": "f4b95099-25d6-424e-b6ec-f9c61e115e19"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.07058824 0.07058824 0.07058824 0.49411765\n",
            "  0.53333333 0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.11764706 0.14117647 0.36862745 0.60392157 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.98431373 0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549 0.96862745\n",
            "  0.94509804 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.31372549 0.61176471 0.41960784 0.99215686 0.99215686 0.80392157 0.04313725 0.         0.16862745\n",
            "  0.60392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.54509804 0.99215686 0.74509804 0.00784314 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04313725 0.74509804 0.99215686 0.2745098  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.1372549  0.94509804 0.88235294 0.62745098 0.42352941\n",
            "  0.00392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.31764706 0.94117647 0.99215686 0.99215686\n",
            "  0.46666667 0.09803922 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.17647059 0.72941176 0.99215686\n",
            "  0.99215686 0.58823529 0.10588235 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0627451  0.36470588\n",
            "  0.98823529 0.99215686 0.73333333 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.97647059 0.99215686 0.97647059 0.25098039 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.18039216 0.50980392 0.71764706\n",
            "  0.99215686 0.99215686 0.81176471 0.00784314 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.15294118 0.58039216 0.89803922 0.99215686 0.99215686\n",
            "  0.99215686 0.98039216 0.71372549 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.78823529 0.30588235 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.09019608 0.25882353 0.83529412 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706\n",
            "  0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588 0.31372549 0.03529412 0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.53333333 0.99215686 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Build machine learning model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "_Q5RlFIvk3iB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 For each example, the model returns a vector of logits or log-odds scores, one for each class.\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM7eI3FKnWlf",
        "outputId": "6cd7690b-4ab7-4cec-920f-043f85f38da1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.70219475,  0.6030649 , -0.6134447 ,  0.5723269 , -0.11557727, -0.38665807, -0.6237098 ,  0.6774872 , -0.3198744 ,  0.34265694]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 The tf.nn.softmax function converts these logits to probabilities for each class\n",
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwzAyoC7n3GV",
        "outputId": "d253ac20-a466-41bb-9c04-5f56243c40e7"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.16315444, 0.14775676, 0.04377477, 0.1432841 , 0.07201866, 0.05491817, 0.04332772, 0.15917268, 0.05871106, 0.11388162]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Define a loss function for training using losses.SparseCategoricalCrossentropy,\n",
        "# which takes a vector of logits and a True index and returns a scalar loss for each example.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "ugtsy4a8oagx"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
        "# This untrained model gives probabilities close to random (1/10 for each class), \n",
        "# so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\n",
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5G-X4KWp-zT",
        "outputId": "f3e27621-cfb3-41ce-c068-54e4b727629d"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.901911"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Configure and compile the model \n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=loss_fn,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "EWzXu-OcqjcW"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate the model\n"
      ],
      "metadata": {
        "id": "73obEdgPrONt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Adjust model parameters and minimize loss\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZh7dggFrVcW",
        "outputId": "400fccc9-c7b6-43e5-f11b-a9b346b2b469"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 5ms/step - loss: 0.2931 - accuracy: 0.9154\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1435 - accuracy: 0.9576\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1089 - accuracy: 0.9672\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0896 - accuracy: 0.9727\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0772 - accuracy: 0.9759\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f619f54bbd0>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Check the model performance \n",
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_eP2je8nD3E",
        "outputId": "8c55f8fd-c44b-49fa-9fea-a2c3729b9ada"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0724 - accuracy: 0.9787 - 528ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07244972884654999, 0.9786999821662903]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# => Image classifier accuracy ~98% on this dataset"
      ],
      "metadata": {
        "id": "Q_hsgXqQtFQQ"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Return probability of the matching for a specific test\n",
        "probability_model = tf.keras.Sequential([\n",
        "    model, \n",
        "    tf.keras.layers.Softmax()\n",
        "])"
      ],
      "metadata": {
        "id": "_YVodfq3ukGH"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = probability_model(x_test[:5])\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKL1COGvpkZ",
        "outputId": "6b567c21-16ed-4415-d4c1-a5915de3ba5b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[7.6787225e-09, 2.7699626e-10, 5.4135648e-07, 3.7867695e-04, 3.8468157e-12, 1.8178040e-08, 1.1371305e-14, 9.9962044e-01, 3.4988272e-08, 2.5746786e-07],\n",
              "       [1.3154626e-08, 6.3927100e-06, 9.9995577e-01, 3.7519050e-05, 9.8700658e-15, 8.9645745e-08, 1.2340456e-08, 2.4697673e-13, 2.4584435e-07, 8.8565914e-14],\n",
              "       [4.5221412e-08, 9.9913675e-01, 1.8753226e-04, 4.6276073e-06, 3.5296907e-05, 4.6460514e-06, 3.1122541e-05, 4.9425772e-04, 1.0558977e-04, 8.6332641e-08],\n",
              "       [9.9999058e-01, 9.6276313e-11, 3.1919242e-07, 3.3577759e-08, 1.8807450e-08, 9.4122890e-07, 3.1348638e-06, 2.5125998e-06, 7.3875822e-10, 2.5200943e-06],\n",
              "       [3.8903718e-06, 5.2191829e-09, 2.0809744e-06, 5.5007945e-08, 9.9801612e-01, 3.7057265e-07, 1.3227692e-06, 2.4977172e-04, 2.2496009e-07, 1.7260549e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rounded_resultst = tf.math.round(results)\n",
        "rounded_resultst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVOwPax37rNu",
        "outputId": "6a028ccc-e6cc-407a-fc33-c10d78cd80b6"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Suggestion for results = [7, 2, 1, 0, 4]\n",
        "# How to show the results in numbers automatically?\n",
        "# How to compare the results?\n",
        "y_test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BaEPvBWwJCZ",
        "outputId": "7579eb9f-8101-48c3-ed40-0575c42f9ecc"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Exercise"
      ],
      "metadata": {
        "id": "YShLWZuA0RIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xpoKNpIE1MDk"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Define and compile neural network\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # 1 layer / 1 neuron"
      ],
      "metadata": {
        "id": "0Nx5gKSO0nyn"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Compile NN with 2 functions: Loss and optimizer\n",
        "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")"
      ],
      "metadata": {
        "id": "__ckCS-r1T9d"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Providing data\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
      ],
      "metadata": {
        "id": "q_M7Hoh-4Ve-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Training NN\n",
        "model.fit(xs, ys, epochs=500)\n",
        "print(model.predict([10.0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQzzq7kJ4tA-",
        "outputId": "b91f244f-88d6-4388-9660-9cea08bd45a9"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 283ms/step - loss: 5.3749\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.4065\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.6410\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 3.0352\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 2.5550\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.1738\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8706\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.6287\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4351\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2797\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1543\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0527\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.9697\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9016\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.8451\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7978\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7580\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7239\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6945\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6688\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6460\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6257\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6073\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5904\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5748\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5603\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5467\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5338\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5215\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.5097\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4984\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4875\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4770\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4668\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4569\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4473\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4379\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4288\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4198\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4111\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4026\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3943\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.3861\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3782\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3704\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3627\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3553\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3480\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3408\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3338\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.3269\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3202\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3136\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3072\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.3009\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2947\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2886\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2827\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2769\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2712\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2656\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2602\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2548\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.2496\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2445\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2394\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2345\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2297\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2250\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2204\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2158\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2114\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2071\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2028\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1986\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1946\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1906\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1867\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1828\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1791\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.1754\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1718\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1683\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1648\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1614\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1581\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1549\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1517\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1486\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1455\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1425\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1396\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1367\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1339\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1312\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1285\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1258\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1232\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1207\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1182\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1158\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1134\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1111\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1088\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1066\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1044\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1022\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1001\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0981\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0961\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0941\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0922\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0903\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0884\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0866\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0848\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0831\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0814\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0797\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0781\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0765\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0749\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0734\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0718\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0704\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0689\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0675\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0661\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0648\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0634\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0621\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0609\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0596\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0584\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0572\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0560\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0549\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0537\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0526\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0515\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0505\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0495\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0484\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0474\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0465\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0455\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0446\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0437\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0428\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0419\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0410\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0402\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0394\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0385\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0378\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 192ms/step - loss: 0.0370\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0362\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0355\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0347\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0340\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0333\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0327\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0320\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0313\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0307\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0301\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0294\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0288\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0282\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0277\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0271\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0265\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0260\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0255\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0249\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0244\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0239\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0234\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0229\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0225\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0220\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0216\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0211\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0207\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0203\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0198\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0194\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0190\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0186\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0183\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0179\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0175\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0172\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0168\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0165\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0161\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0158\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0155\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0151\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0148\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0145\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0142\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0139\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0137\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0134\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0131\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0128\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0126\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0123\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0121\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0118\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0116\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0113\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0111\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0109\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0106\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0104\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0102\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0100\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0098\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0096\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0094\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0092\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0090\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0088\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0087\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0085\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0083\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0081\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0080\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0078\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0076\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0075\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0073\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0072\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0070\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0069\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0067\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 284ms/step - loss: 0.0066\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0065\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0063\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0062\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 28ms/step - loss: 0.0061\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0060\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0058\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0057\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0056\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0055\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0054\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0053\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0051\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0050\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0049\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0048\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0047\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0046\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0045\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0045\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0044\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0043\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0042\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0041\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0040\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0039\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0039\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0038\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0037\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0036\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0035\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0035\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0034\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0033\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0032\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0031\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0030\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0029\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0029\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0028\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0028\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0027\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.0027\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0026\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0025\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0025\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0024\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0023\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0023\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0022\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0022\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0022\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0021\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0021\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0020\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0020\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0019\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0019\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0017\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0016\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0015\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0015\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0014\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0013\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0011\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0011\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0010\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.9914e-04\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.7862e-04\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.5852e-04\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.3883e-04\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.1955e-04\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.0066e-04\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.8216e-04\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.6404e-04\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.4629e-04\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2891e-04\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.1188e-04\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9521e-04\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7887e-04\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.6287e-04\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4720e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.3185e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1682e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0209e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8767e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7355e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5972e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4616e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.3289e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.1989e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.0716e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.9469e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.8247e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.7051e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.5879e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.4731e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.3607e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.2505e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.1427e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0371e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.9336e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.8323e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7330e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.6358e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5406e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4473e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.3560e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.2665e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.1789e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0930e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.0090e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9266e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8460e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7670e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6896e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6138e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.5396e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 3.4669e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.3957e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.3259e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.2576e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1907e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 3.1251e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.0609e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9981e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9365e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8762e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8171e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7592e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7025e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6470e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5927e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 2.5394e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4873e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.4362e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3861e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.3371e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2891e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.2421e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1960e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.1509e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1067e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 261ms/step - loss: 2.0635e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0211e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.9796e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9389e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8991e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8601e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.8218e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.7844e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.7478e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.7119e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6767e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.6423e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.6086e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5755e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5431e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5114e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4804e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4500e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4202e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3910e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.3625e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.3345e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3071e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2802e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2539e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2282e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2029e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1782e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1540e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.1303e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.1071e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0844e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0621e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0403e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.0189e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9800e-05\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7749e-05\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.5742e-05\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.3775e-05\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.1848e-05\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.9962e-05\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.8113e-05\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6303e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.4532e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2795e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.1095e-05\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.9429e-05\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7796e-05\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.6199e-05\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4633e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.3101e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 7.1598e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.0129e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8688e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7276e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.5894e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.4542e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.3216e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.1919e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.0647e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 5.9400e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.8181e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.6986e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 5.5815e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.4669e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.3545e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.2446e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.1369e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5.0314e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.9280e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 4.8268e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7276e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.6306e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 4.5355e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.4423e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3510e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2617e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1742e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0885e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 4.0045e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 3.9222e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.8416e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 3.7627e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 71ms/step - loss: 3.6854e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6097e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.5356e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.4630e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3918e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3221e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2539e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.1871e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.1216e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.0575e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9947e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9331e-05\n",
            "[[18.9842]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Exercise"
      ],
      "metadata": {
        "id": "LRoqLzxB9gU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you'll try to build a neural network that predicts the price of a house according to a simple formula.\n",
        "\n",
        "So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.\n",
        "\n",
        "How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.\n",
        "\n",
        "Hint: Your network might work better if you scale the house price down. You don't have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the 'hundreds of thousands' etc."
      ],
      "metadata": {
        "id": "-hfRrFTf936S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: house_model\n",
        "def house_model(y_new):\n",
        "    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0 , 6.0], dtype=float)\n",
        "    y = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float)\n",
        "    model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "    model.compile(optimizer=\"SGD\", loss=\"mse\")\n",
        "    model.fit(x,y, epochs=500)\n",
        "    return model.predict(y_new)[0]"
      ],
      "metadata": {
        "id": "fcmFakXB9on3"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = house_model([7])\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixnrSD1VCYt-",
        "outputId": "68d44559-a9c4-4083-9af6-61477c0220fa"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 389ms/step - loss: 55784.9258\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 25955.4824\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12148.5205\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 5757.2583\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2798.2107\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1427.7017\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 792.4217\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 497.4364\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 359.9561\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 295.3810\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 264.5556\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 249.3571\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 241.3982\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 236.7968\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 233.7559\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 231.4439\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 229.4756\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 227.6734\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 225.9544\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 224.2802\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 222.6334\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 221.0052\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 219.3923\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 217.7926\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 216.2053\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 214.6298\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 213.0661\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 211.5137\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 209.9727\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 208.4429\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 206.9244\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 205.4166\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 203.9202\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 202.4343\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 200.9595\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 199.4954\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 198.0420\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 196.5991\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 195.1668\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 193.7449\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 192.3336\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 190.9323\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 189.5412\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 188.1603\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 186.7894\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 185.4285\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 184.0775\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 182.7365\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 181.4052\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 180.0836\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 178.7716\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 177.4690\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 176.1760\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 174.8926\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 173.6184\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 172.3535\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 171.0978\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 169.8512\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 168.6139\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 167.3854\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 166.1657\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 164.9554\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 163.7535\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 162.5603\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 161.3760\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 160.2003\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 159.0333\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 157.8746\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 156.7244\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 155.5825\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 154.4491\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 153.3237\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 152.2067\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 151.0979\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 149.9970\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 148.9040\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 147.8193\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 146.7423\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 145.6734\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 144.6120\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.5584\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 142.5124\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 141.4743\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 140.4434\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 139.4203\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 138.4046\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 137.3962\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 136.3951\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 135.4014\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 134.4150\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 133.4357\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 132.4635\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 131.4985\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 130.5405\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 129.5893\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 128.6454\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 127.7081\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 126.7776\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 125.8540\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 124.9370\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 124.0268\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 123.1231\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 122.2261\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 121.3356\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 120.4516\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 119.5741\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 118.7029\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 117.8381\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 116.9796\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 116.1274\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 115.2812\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 114.4414\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 113.6077\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 112.7799\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 111.9583\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 111.1426\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 110.3329\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 109.5289\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 108.7309\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 107.9388\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 107.1524\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 106.3717\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 105.5968\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 104.8274\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 104.0637\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 103.3055\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 102.5529\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 101.8057\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 101.0641\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 100.3277\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 99.5969\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 98.8712\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 98.1509\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 97.4358\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 96.7260\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 96.0212\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 95.3216\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 94.6272\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 93.9378\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 93.2534\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 92.5739\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 91.8995\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 91.2300\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 90.5652\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 89.9055\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 89.2505\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 88.6002\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 87.9548\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 87.3140\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 86.6778\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 86.0463\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 85.4194\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 84.7970\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 84.1793\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 83.5660\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 82.9571\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 82.3527\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 81.7528\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 81.1572\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 80.5658\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 79.9788\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 79.3961\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 78.8178\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 78.2436\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 77.6735\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 77.1076\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 76.5458\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 75.9881\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 75.4346\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 74.8850\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 74.3395\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 73.7978\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 73.2601\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 72.7264\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 72.1966\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 71.6705\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 71.1483\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 70.6301\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 70.1155\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 69.6046\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 69.0976\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 68.5941\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 68.0944\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 67.5983\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 67.1058\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 66.6168\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 66.1316\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 65.6498\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 65.1715\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 64.6966\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 64.2253\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 63.7574\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 63.2928\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 62.8317\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 62.3740\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 61.9195\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 61.4684\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 61.0206\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 60.5760\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 60.1347\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 59.6966\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 59.2616\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 58.8299\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 58.4013\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 57.9757\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 57.5534\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 57.1341\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 56.7178\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 56.3046\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 55.8943\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 55.4871\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 55.0828\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 54.6816\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 54.2832\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 53.8878\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 53.4952\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 53.1054\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 52.7184\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 52.3344\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 51.9532\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 51.5747\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 51.1988\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 50.8259\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 50.4556\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 50.0880\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 49.7230\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 49.3608\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 49.0012\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 48.6442\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 48.2898\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 47.9380\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 47.5887\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 47.2420\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.8978\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.5562\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.2170\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 295ms/step - loss: 45.8802\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 45.5460\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 45.2141\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 44.8847\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 44.5578\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 44.2330\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 43.9108\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 43.5909\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 43.2733\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 42.9580\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 42.6451\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 42.3344\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 42.0260\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 41.7198\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 41.4158\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 41.1140\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 40.8145\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 40.5171\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 40.2220\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 39.9289\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 39.6380\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 39.3492\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 39.0626\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 38.7780\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 38.4955\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 38.2149\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.9366\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.6601\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.3858\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.1134\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.8430\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.5746\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.3081\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.0436\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.7811\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.5203\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.2616\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.0046\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 34.7497\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 34.4965\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 34.2452\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.9957\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.7480\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.5021\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.2580\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 33.0157\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 32.7752\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 32.5364\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 32.2994\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 32.0640\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 31.8304\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 31.5986\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 31.3683\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 31.1398\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 30.9130\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 30.6876\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 30.4641\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 30.2422\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 30.0218\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 29.8031\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 29.5860\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 29.3704\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 29.1564\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 28.9440\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.7332\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.5238\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.3160\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 28.1097\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 27.9049\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 27.7016\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 27.4997\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 27.2994\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 27.1005\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 26.9031\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 26.7070\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 26.5125\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 26.3193\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 26.1276\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 25.9373\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 25.7483\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 25.5607\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 25.3745\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 25.1896\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 177ms/step - loss: 25.0060\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 24.8239\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 24.6431\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 24.4635\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 24.2852\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 24.1084\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 23.9327\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 23.7583\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 23.5853\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 23.4134\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 23.2429\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 23.0735\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 22.9054\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 22.7385\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 22.5729\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 22.4083\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 22.2451\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 22.0831\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 21.9222\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 21.7624\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 21.6039\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 21.4465\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 21.2902\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 21.1351\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 20.9812\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 20.8283\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 20.6766\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 20.5259\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 20.3763\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 20.2279\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 20.0805\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 19.9342\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 19.7890\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 19.6448\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 19.5017\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 19.3596\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 19.2185\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 19.0785\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 18.9395\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 18.8015\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.6645\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.5286\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 18.3936\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 18.2596\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.1266\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.9944\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 17.8634\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.7332\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.6040\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 17.4758\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 17.3484\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 17.2221\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 17.0966\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 16.9720\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 16.8484\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 16.7256\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 16.6038\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 16.4828\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.3627\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.2435\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 16.1252\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.0076\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 15.8910\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.7752\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.6604\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 15.5462\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.4330\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 15.3205\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.2089\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.0981\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.9881\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.8789\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.7705\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 14.6629\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 14.5561\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.4500\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 14.3447\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.2402\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.1365\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 14.0335\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 13.9312\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 13.8298\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.7290\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.6290\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.5297\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 13.4311\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.3333\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 13.2361\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.1397\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13.0440\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.9489\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.8546\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.7609\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.6680\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.5757\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.4841\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.3931\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.3028\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.2132\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 12.1242\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.0359\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 11.9482\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.8611\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.7747\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.6889\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.6037\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 11.5192\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.4353\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.3520\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.2693\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 11.1872\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.1056\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.0248\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.9444\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.8647\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.7855\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.7070\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.6290\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.5515\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.4747\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 10.3984\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 10.3226\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.2474\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 10.1727\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.0986\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.0251\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.9520\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.8795\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.8075\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7361\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.6651\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.5947\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.5248\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4554\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.3865\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.3182\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.2503\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.1829\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.1160\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.0496\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.9836\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.9182\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.8532\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.7887\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.7247\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.6611\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.5980\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.5354\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.4732\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.4115\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.3502\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.2893\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.2289\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.1690\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.1095\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.0504\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.9917\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 7.9335\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.8757\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.8183\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.7614\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7048\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.6487\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.5930\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5376\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4827\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.4282\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 98ms/step - loss: 7.3741\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.3204\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.2670\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2141\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1615\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.1094\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.0576\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.0061\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.9551\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 6.9044\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.8541\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.8042\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.7546\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.7054\n",
            "[403.7351]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Excerise"
      ],
      "metadata": {
        "id": "HIKpPJE8cZQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load fashion dataset\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZDBjcahdAa7",
        "outputId": "0519ff94-e294-4f13-d520-c7aceb07af29"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "plt.imshow(training_images[0])\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 768
        },
        "id": "7SvhecN2gLPs",
        "outputId": "6de332e9-be99-4200-c873-cb6912b37983"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize data \n",
        "training_images = training_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "id": "-_ibD6PcgyEv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.cbook import flatten\n",
        "#Design model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                   tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                                   tf.keras.layers.Dense(10, activation=\"softmax\")])"
      ],
      "metadata": {
        "id": "xLdSHsRkg95m"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss= \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "model.fit(training_images, training_labels, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWUODQIoifEG",
        "outputId": "44482bc5-cac1-40f3-9110-41405c188572"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 3ms/step - loss: 0.4980 - accuracy: 0.8245\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3715 - accuracy: 0.8659\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.3333 - accuracy: 0.8779\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3110 - accuracy: 0.8853\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.2937 - accuracy: 0.8910\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f619f325d10>"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0UV159Qk1c5",
        "outputId": "387d140e-f619-42bf-c14c-b98ff66a389e"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3395 - accuracy: 0.8789\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.33952754735946655, 0.8788999915122986]"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(tf.math.round(classifications[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-mxud3VlZzR",
        "outputId": "fd5428a7-b54f-4093-923c-8c72358e0527"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(5, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction [9,2,1,1,6]\n",
        "test_labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJguULpnu1R",
        "outputId": "58162406-28e2-4e40-ebee-f7b0eadf2ecf"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Outmate classification\n",
        "output = tf.gather(tf.math.round(classifications[:5]), 0)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOZse-cRrbAH",
        "outputId": "2da108a9-1bc8-4130-e243-a2fffdbe0dbe"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_nine = tf.constant([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
        "class_nine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv-Svdsmu7eB",
        "outputId": "cacff635-34bd-4c63-99a8-07797e214894"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def classification_automation(output):\n",
        "#   if output ==  np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32):\n",
        "#     print(\"class 09\") \n",
        "# => If statements do not work for tensorflow"
      ],
      "metadata": {
        "id": "r8twKsRYr3rw"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.math.equal(class_nine, output)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7thBrGKx0dr",
        "outputId": "33c7b979-e70d-435d-ec58-772ffe20555a"
      },
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_eight = tf.constant([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
        "b = tf.math.equal(class_eight, output)\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cnWuuRi-W1v",
        "outputId": "b8fd2286-7db6-4554-92c5-c290cb1e5fb8"
      },
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True, False, False])>"
            ]
          },
          "metadata": {},
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.numpy()) #=> correct class\n",
        "print(b.numpy()) #=> wrong class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-vyAxdSAsR6",
        "outputId": "df7af784-9964-4d7f-8007-966be638ca95"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True  True  True  True  True  True  True  True  True]\n",
            "[ True  True  True  True  True  True  True  True False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Ex"
      ],
      "metadata": {
        "id": "xEdJlqsKCBKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now test with 512 neurons\n",
        "#What different results do you get for loss, training time etc? \n",
        "#Why do you think that's the case?\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viOkXZfpCEB-",
        "outputId": "14fda06f-3358-4ebe-a16a-164ea8850ecf"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.4752 - accuracy: 0.8294\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3578 - accuracy: 0.8692\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.3224 - accuracy: 0.8821\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2964 - accuracy: 0.8907\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.2785 - accuracy: 0.8959\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3737 - accuracy: 0.8656\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy8a_RuMH1ES",
        "outputId": "dda61685-446a-4d36-9b01-d274d678f913"
      },
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hypothesis: Double the amount of neurons will increase accuracy\n",
        "#But processing will take more time \n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdwifcFcINAC",
        "outputId": "0e407dae-c7a7-419b-d597-ee97f4387249"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.4709 - accuracy: 0.8298\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.3568 - accuracy: 0.8700\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 14s 8ms/step - loss: 0.3201 - accuracy: 0.8811\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 13s 7ms/step - loss: 0.2975 - accuracy: 0.8905\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 14s 7ms/step - loss: 0.2780 - accuracy: 0.8952\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3465 - accuracy: 0.8757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result: Training takes longer and accuracy is increased from 0.8656 to 0.8757\n",
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrfGX58nJXaF",
        "outputId": "1a36501b-352e-42c6-bc4c-9234423dd5f7"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Ex"
      ],
      "metadata": {
        "id": "5TM3gP1YCKlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What would happen if you remove the Flatten() layer. Why do you think that's the case?\n",
        "\n",
        "#=> Error: Because shape of data. \n",
        "#RoT = First Layer should be in same shape as input data \n",
        "#With Flatten we create of out 28x28 matrix an array of 784X1\n"
      ],
      "metadata": {
        "id": "xLCWk4JyCPCc"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Ex"
      ],
      "metadata": {
        "id": "nESok22sCONn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the final (output) layers. Why are there 10 of them? \n",
        "#What would happen if you had a different amount than 10? For example, try training the network with 5.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "PJb3ydI7CTPa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 Ex"
      ],
      "metadata": {
        "id": "0ddP6xKNCUDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the effects of additional layers in the network. \n",
        "#What will happen if you add another layer between the one with 512 and the final layer with 10.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "_8gw2Jp9CbDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 Ex"
      ],
      "metadata": {
        "id": "tJ3meIPJCYyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the impact of training for more or less epochs. Why do you think that would be the case?\n",
        "\n",
        "#=>\n"
      ],
      "metadata": {
        "id": "BDs6n82hCYT6"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Ex"
      ],
      "metadata": {
        "id": "QCWoo1T5XTd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. \n",
        "#What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?\n",
        "\n",
        "#=>\n",
        "\n"
      ],
      "metadata": {
        "id": "5sOEqevvXWeL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Ex"
      ],
      "metadata": {
        "id": "Xpa-faKNXjIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier when you trained for extra epochs you had an issue where your loss might change. It might have taken a bit of time for you to wait for the training to do that, and you might have thought 'wouldn't it be nice if I could stop the training when I reach a desired value?' -- i.e. 95% accuracy might be enough for you, and if you reach that after 3 epochs, why sit around waiting for it to finish a lot more epochs....So how would you fix that? Like any other program...you have callbacks! Let's see them in action..."
      ],
      "metadata": {
        "id": "o1cHN4d1XwRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get('accuracy') >= 0.6): # Experiment with changing this value\n",
        "      print(\"\\nReached 60% accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "\n",
        "callbacks = myCallback()\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images=training_images/255.0\n",
        "test_images=test_images/255.0\n",
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(512, activation=tf.nn.relu),\n",
        "  tf.keras.layers.Dense(10, activation=tf.nn.softmax)\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=5, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QE0_XcVBXlqU",
        "outputId": "212e7334-e904-48f9-a2c6-a08cbfa9027e"
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1866/1875 [============================>.] - ETA: 0s - loss: 0.4754 - accuracy: 0.8308\n",
            "Reached 60% accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 8s 4ms/step - loss: 0.4753 - accuracy: 0.8307\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f619c49c690>"
            ]
          },
          "metadata": {},
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "wq5fFB2uXVpR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2: TF.ComputerVision (CV)"
      ],
      "metadata": {
        "id": "j3GUyO_Df9l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKHDSeb8gBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 3: TF.Natrual Language Processing (NLP)"
      ],
      "metadata": {
        "id": "YsyqqzzVgCdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yGrzdqAjgIYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 4: TF.Time Series Forcasting"
      ],
      "metadata": {
        "id": "1AnDmEu3gMew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MKNNViULgUSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}