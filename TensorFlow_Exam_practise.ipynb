{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow - Exam practise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ueEq3n3_gd1W",
        "YShLWZuA0RIv",
        "LRoqLzxB9gU9",
        "xEdJlqsKCBKt",
        "5TM3gP1YCKlE",
        "nESok22sCONn",
        "0ddP6xKNCUDh",
        "tJ3meIPJCYyZ",
        "QCWoo1T5XTd6",
        "1AnDmEu3gMew"
      ],
      "authorship_tag": "ABX9TyOtboR/bIo87Raws8KvVO7F",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reneSalmon/Google_TensorFlow_Certificate/blob/master/TensorFlow_Exam_practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1: TF.Basics\n"
      ],
      "metadata": {
        "id": "9LEN1H22f2zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Quickstart for Beginners"
      ],
      "metadata": {
        "id": "ueEq3n3_gd1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a prebuild dataset\n",
        "2. Build neural network that classifies images\n",
        "3. Train neural network\n",
        "4. Evaluat accuracy of model"
      ],
      "metadata": {
        "id": "AjGO40BLhEam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfCGSB4ahD98",
        "outputId": "a9359e40-2cf5-419c-9641-9a30b822834a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTgTW33HfwCM",
        "outputId": "beea5957-d039-4563-e738-1a12d5af6a45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 1 Load dataset\n",
        "minst = tf.keras.datasets.mnist #load dataset of 70.000 handwriting digits for image classification\n",
        "(x_train, y_train), (x_test, y_test) = minst.load_data() #split dataset x = images, y = image labels\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 #normalize dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])\n",
        "print(y_train[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7rww5zcfeHpb",
        "outputId": "f75c2bbb-7a98-4430-cbbd-a5b154cf81c9"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.07058824 0.07058824 0.07058824 0.49411765\n",
            "  0.53333333 0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.11764706 0.14117647 0.36862745 0.60392157 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.98431373 0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549 0.96862745\n",
            "  0.94509804 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.31372549 0.61176471 0.41960784 0.99215686 0.99215686 0.80392157 0.04313725 0.         0.16862745\n",
            "  0.60392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.54509804 0.99215686 0.74509804 0.00784314 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04313725 0.74509804 0.99215686 0.2745098  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.1372549  0.94509804 0.88235294 0.62745098 0.42352941\n",
            "  0.00392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.31764706 0.94117647 0.99215686 0.99215686\n",
            "  0.46666667 0.09803922 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.17647059 0.72941176 0.99215686\n",
            "  0.99215686 0.58823529 0.10588235 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0627451  0.36470588\n",
            "  0.98823529 0.99215686 0.73333333 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.97647059 0.99215686 0.97647059 0.25098039 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.18039216 0.50980392 0.71764706\n",
            "  0.99215686 0.99215686 0.81176471 0.00784314 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.15294118 0.58039216 0.89803922 0.99215686 0.99215686\n",
            "  0.99215686 0.98039216 0.71372549 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.78823529 0.30588235 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.09019608 0.25882353 0.83529412 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706\n",
            "  0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588 0.31372549 0.03529412 0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.53333333 0.99215686 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Build machine learning model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "_Q5RlFIvk3iB"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 For each example, the model returns a vector of logits or log-odds scores, one for each class.\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM7eI3FKnWlf",
        "outputId": "c921bd69-81a6-4889-9b46-a8aacaf422cd"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.41706666,  0.21360788,  0.61776614, -0.18589236,  0.2977742 ,  0.9369845 ,  0.40918368, -0.41811752, -0.15437573, -0.4570311 ]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 The tf.nn.softmax function converts these logits to probabilities for each class\n",
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwzAyoC7n3GV",
        "outputId": "5dabc16d-8e7e-4680-d8c5-71488a07d56d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.05430217, 0.10202713, 0.15284082, 0.06842501, 0.1109861 , 0.21031688, 0.12406611, 0.05424514, 0.07061589, 0.05217481]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Define a loss function for training using losses.SparseCategoricalCrossentropy,\n",
        "# which takes a vector of logits and a True index and returns a scalar loss for each example.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "ugtsy4a8oagx"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
        "# This untrained model gives probabilities close to random (1/10 for each class), \n",
        "# so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\n",
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5G-X4KWp-zT",
        "outputId": "9bf0a3d5-8fc7-4917-e60d-c8d5505691f6"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.55914"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Configure and compile the model \n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=loss_fn,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "EWzXu-OcqjcW"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate the model\n"
      ],
      "metadata": {
        "id": "73obEdgPrONt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Adjust model parameters and minimize loss\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZh7dggFrVcW",
        "outputId": "8ab71148-7248-4099-fe1e-9fb1de3a3b4b"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 9s 4ms/step - loss: 0.2952 - accuracy: 0.9150\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.1432 - accuracy: 0.9571\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.1075 - accuracy: 0.9674\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 9s 5ms/step - loss: 0.0874 - accuracy: 0.9729\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0759 - accuracy: 0.9765\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc6adef3350>"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Check the model performance \n",
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_eP2je8nD3E",
        "outputId": "ed26c631-5bac-4138-a0ec-ba305a3571cb"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 0s - loss: 0.0730 - accuracy: 0.9778 - 492ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07295425981283188, 0.9778000116348267]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# => Image classifier accuracy ~98% on this dataset"
      ],
      "metadata": {
        "id": "Q_hsgXqQtFQQ"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Return probability of the matching for a specific test\n",
        "probability_model = tf.keras.Sequential([\n",
        "    model, \n",
        "    tf.keras.layers.Softmax()\n",
        "])"
      ],
      "metadata": {
        "id": "_YVodfq3ukGH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = probability_model(x_test[:5])\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKL1COGvpkZ",
        "outputId": "90ef30ca-3899-4bc1-d3a3-65b845b6d9df"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[3.3969467e-08, 2.6737265e-10, 5.4452579e-07, 3.5805231e-05, 8.6333433e-11, 9.4856865e-08, 2.8518547e-15, 9.9996161e-01, 4.5188085e-07, 1.4296792e-06],\n",
              "       [2.9531209e-09, 3.1759449e-05, 9.9994898e-01, 1.8726945e-05, 4.0165574e-15, 3.2062798e-07, 2.9143118e-08, 1.9182715e-12, 2.2308937e-07, 9.1211155e-16],\n",
              "       [4.7222802e-08, 9.9937856e-01, 5.3546264e-05, 1.7638815e-05, 2.8318475e-05, 7.8231396e-06, 1.1662287e-06, 3.4896526e-04, 1.6366056e-04, 2.7336316e-07],\n",
              "       [9.9996614e-01, 8.6110707e-10, 4.8519462e-07, 8.4208782e-09, 3.3583552e-07, 3.3879585e-06, 1.3475180e-05, 1.5677359e-05, 1.1923207e-08, 4.3788427e-07],\n",
              "       [3.9206884e-06, 6.0962448e-09, 8.7183016e-06, 1.5521921e-07, 9.9736494e-01, 2.4079568e-06, 4.1354451e-06, 7.8948549e-05, 5.0949861e-06, 2.5316055e-03]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rounded_resultst = tf.math.round(results)\n",
        "rounded_resultst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVOwPax37rNu",
        "outputId": "8b944a39-5cd5-4a60-f60e-ed4014d47df5"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Suggestion for results = [7, 2, 1, 0, 4]\n",
        "# How to show the results in numbers automatically?\n",
        "# How to compare the results?\n",
        "y_test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BaEPvBWwJCZ",
        "outputId": "6abb9bb0-22bc-4eb2-bcef-2760241736bc"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Exercise"
      ],
      "metadata": {
        "id": "YShLWZuA0RIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xpoKNpIE1MDk"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Define and compile neural network\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # 1 layer / 1 neuron"
      ],
      "metadata": {
        "id": "0Nx5gKSO0nyn"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Compile NN with 2 functions: Loss and optimizer\n",
        "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")"
      ],
      "metadata": {
        "id": "__ckCS-r1T9d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Providing data\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
      ],
      "metadata": {
        "id": "q_M7Hoh-4Ve-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Training NN\n",
        "model.fit(xs, ys, epochs=500)\n",
        "print(model.predict([10.0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQzzq7kJ4tA-",
        "outputId": "2587835d-45e8-4571-998b-fe9e8c5f095c"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 253ms/step - loss: 2.6895\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2683\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.9339\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6677\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.4552\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2851\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1484\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0381\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.9485\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8753\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.8151\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.7651\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.7232\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6878\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6575\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6312\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.6082\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5878\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5696\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5530\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.5378\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5237\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.5106\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4982\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4865\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4754\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4647\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4544\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4445\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4350\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.4257\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4167\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.4079\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3993\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3910\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3829\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3749\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3672\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3596\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3521\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3449\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3378\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3308\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.3240\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3173\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3108\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.3044\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2981\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2920\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2860\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2801\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2744\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.2687\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2632\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2578\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2525\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2473\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2423\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2373\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2324\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2276\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2230\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2184\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2139\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2095\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.2052\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.2010\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1968\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1928\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1888\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1850\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.1812\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1774\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1738\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1702\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1667\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.1633\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1600\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.1567\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1534\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1503\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1472\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1442\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1412\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1383\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1355\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1327\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1300\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1273\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1247\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.1221\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1196\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1172\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1148\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1124\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1101\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1078\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1056\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.1034\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1013\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0992\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0972\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0952\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0932\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0913\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0895\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0876\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0858\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0841\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0823\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0806\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0790\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0774\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0758\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0742\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0727\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0712\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0697\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0683\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0669\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0655\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 106ms/step - loss: 0.0642\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0629\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0616\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0603\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0591\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0579\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0567\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0555\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0544\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0532\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0522\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0511\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0500\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0490\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0480\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0470\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0460\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0451\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0442\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0433\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0424\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0415\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0407\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0398\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0390\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0382\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0374\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0366\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0359\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0352\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0344\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0337\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0330\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0324\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0317\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0310\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0304\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0298\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0292\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0286\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0280\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0274\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0268\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0263\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0258\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0252\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0247\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0242\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0237\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0232\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0227\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0223\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0218\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0214\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0209\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0205\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0201\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0197\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0193\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0189\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0185\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0181\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0177\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0174\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0170\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0167\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0163\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0160\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0156\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0153\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0150\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0144\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0141\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0138\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0135\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0133\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0130\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0127\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0125\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0122\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0119\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0117\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 325ms/step - loss: 0.0115\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0112\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0110\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0108\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0105\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0103\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0101\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0099\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0097\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0095\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0093\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0091\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0089\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0088\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0086\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0084\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0082\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0081\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0079\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0077\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0076\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0074\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0073\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0071\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0070\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0068\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0067\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0065\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0064\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0063\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0061\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0060\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0059\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0058\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0057\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0055\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0054\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 20ms/step - loss: 0.0053\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0052\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0051\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0050\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0049\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0048\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0047\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0046\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0045\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0044\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0043\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0042\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0040\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0039\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0038\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0037\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0037\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0036\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0035\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0034\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0034\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0033\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0031\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0030\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0029\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0028\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0027\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0026\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0025\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0025\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0024\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0023\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0023\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0022\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0022\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0021\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0021\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0020\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0020\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0020\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0019\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0019\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0018\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0017\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0017\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0016\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0016\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0016\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0015\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0015\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0014\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0014\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0014\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0013\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0013\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0012\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0012\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0012\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0012\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0011\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0011\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0011\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0010\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9006e-04\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.6972e-04\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.4980e-04\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.3029e-04\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.1118e-04\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 8.9247e-04\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.7413e-04\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.5618e-04\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.3859e-04\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.2137e-04\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.0450e-04\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.8797e-04\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.7179e-04\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5593e-04\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4041e-04\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2520e-04\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1031e-04\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9571e-04\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8142e-04\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6743e-04\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5372e-04\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4029e-04\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2714e-04\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.1426e-04\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0164e-04\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.8928e-04\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.7718e-04\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.6532e-04\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.5371e-04\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.4233e-04\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.3119e-04\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.2028e-04\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0959e-04\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.9913e-04\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8887e-04\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.7883e-04\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.6900e-04\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.5936e-04\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4993e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.4069e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.3164e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 4.2277e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 4.1408e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 4.0558e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 3.9725e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 3.8908e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8109e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7327e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6560e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 190ms/step - loss: 3.5809e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5073e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.4353e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3647e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.2956e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.2279e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1616e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0967e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0330e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9707e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9097e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8499e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7914e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.7341e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.6779e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6229e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.5690e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.5163e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.4646e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.4140e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2.3644e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.3158e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 2.2682e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 2.2217e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.1760e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.1313e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 2.0875e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0446e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 2.0026e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.9615e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.9212e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8818e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.8431e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.8052e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7682e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7318e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.6963e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6614e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.6273e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.5939e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5611e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.5291e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.4977e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4669e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4368e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4072e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.3783e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 1.3500e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.3223e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2951e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2685e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 1.2425e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.2170e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1920e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1675e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1435e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.1200e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 1.0970e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.0745e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0524e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0308e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0096e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.8889e-05\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.6856e-05\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.4866e-05\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2918e-05\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.1008e-05\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.9139e-05\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.7308e-05\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.5516e-05\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.3759e-05\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2038e-05\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0352e-05\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.8701e-05\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.7086e-05\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.5503e-05\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.3951e-05\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.2432e-05\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.0944e-05\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9488e-05\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8060e-05\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.6663e-05\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.5292e-05\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 155ms/step - loss: 6.3951e-05\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.2637e-05\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.1351e-05\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.0091e-05\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.8857e-05\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.7648e-05\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.6464e-05\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.5304e-05\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.4168e-05\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.3055e-05\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.1965e-05\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 5.0898e-05\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.9851e-05\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.8828e-05\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.7825e-05\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.6843e-05\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.5881e-05\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.4938e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.4014e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.3111e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.2226e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 4.1358e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 4.0508e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.9677e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8862e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.8064e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.7282e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.6516e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.5766e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.5032e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.4311e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.3607e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2916e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2241e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.1579e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.0930e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0294e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 2.9672e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.9063e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.8466e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7881e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7309e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.6748e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.6199e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 2.5660e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.5134e-05\n",
            "[[18.985374]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Exercise"
      ],
      "metadata": {
        "id": "LRoqLzxB9gU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you'll try to build a neural network that predicts the price of a house according to a simple formula.\n",
        "\n",
        "So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.\n",
        "\n",
        "How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.\n",
        "\n",
        "Hint: Your network might work better if you scale the house price down. You don't have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the 'hundreds of thousands' etc."
      ],
      "metadata": {
        "id": "-hfRrFTf936S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: house_model\n",
        "def house_model(y_new):\n",
        "    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0 , 6.0], dtype=float)\n",
        "    y = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float)\n",
        "    model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "    model.compile(optimizer=\"SGD\", loss=\"mse\")\n",
        "    model.fit(x,y, epochs=500)\n",
        "    return model.predict(y_new)[0]"
      ],
      "metadata": {
        "id": "fcmFakXB9on3"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = house_model([7])\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixnrSD1VCYt-",
        "outputId": "43f660f4-3b4f-4709-a3f3-a317b9defe71"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 588ms/step - loss: 58378.3906\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 27153.5078\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12700.7158\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6010.5610\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 2913.1924\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1478.6801\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 813.7957\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 505.1238\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 361.3255\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 293.8420\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 261.6863\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 245.8881\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 237.6670\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 232.9597\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 229.8851\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 227.5728\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 225.6197\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 223.8394\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 222.1456\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 220.4977\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 218.8777\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 217.2769\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 215.6910\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 214.1181\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 212.5578\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 211.0088\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 209.4713\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 207.9449\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 206.4300\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 204.9261\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 203.4332\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 201.9510\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 200.4797\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 199.0189\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 197.5690\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 196.1298\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 194.7009\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 193.2823\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 191.8741\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 190.4762\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 189.0883\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 187.7109\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 186.3432\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 184.9855\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 183.6379\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 182.3000\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 180.9718\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 179.6533\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 178.3444\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 177.0451\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 175.7551\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 174.4748\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 173.2035\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 171.9417\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 170.6892\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 169.4454\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 168.2109\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 166.9855\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 165.7688\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 164.5612\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 163.3622\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 162.1721\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 160.9906\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 159.8176\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 158.6533\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 157.4974\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 156.3498\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 155.2109\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 154.0800\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 152.9575\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 151.8432\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 150.7368\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 149.6386\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 148.5485\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 147.4661\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 146.3919\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 145.3252\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 144.2666\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.2155\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 142.1720\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 141.1363\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 140.1080\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 139.0871\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 138.0738\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 137.0680\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 136.0693\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 135.0780\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 134.0939\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 133.1169\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 132.1471\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 131.1842\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 130.2286\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 129.2798\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 128.3378\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 127.4030\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 126.4747\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 125.5533\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 124.6385\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 123.7305\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 122.8290\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 121.9340\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 121.0458\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 120.1638\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 119.2884\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 118.4194\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 117.5566\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 116.7001\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 115.8499\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 115.0059\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 114.1680\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 113.3361\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 112.5105\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 111.6908\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 110.8770\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 110.0692\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 109.2673\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 108.4712\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 107.6811\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 106.8965\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 106.1177\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 105.3445\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 104.5770\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 103.8151\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 103.0588\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 102.3079\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 101.5625\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 100.8227\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 100.0880\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 99.3589\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 98.6350\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 97.9163\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 97.2030\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 96.4948\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 95.7919\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 95.0940\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 94.4011\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 93.7133\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 93.0306\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 92.3528\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 91.6800\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 91.0120\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 90.3490\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 89.6907\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 89.0373\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 88.3885\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 87.7447\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 87.1054\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 86.4707\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 85.8408\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 85.2154\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 84.5945\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 83.9782\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 83.3664\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 82.7590\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 82.1561\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 81.5575\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 80.9633\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 80.3735\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 79.7879\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 79.2066\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 78.6295\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 78.0566\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 77.4880\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 76.9234\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 76.3629\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 75.8067\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 75.2543\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 74.7061\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 74.1618\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 73.6215\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 73.0851\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 72.5527\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 72.0241\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 71.4994\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 70.9784\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 70.4613\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 69.9479\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 69.4383\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 68.9325\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 68.4302\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 67.9316\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 67.4367\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 66.9454\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 66.4578\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 65.9736\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 65.4929\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 65.0157\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 64.5420\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 64.0719\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 63.6051\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 63.1416\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 62.6816\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 62.2249\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 61.7717\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 61.3215\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 60.8748\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 60.4314\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 59.9910\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 59.5540\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 59.1201\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 58.6893\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 58.2617\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 214ms/step - loss: 57.8373\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 57.4158\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 56.9976\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 56.5824\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 56.1701\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 55.7609\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 55.3545\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 54.9514\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 54.5509\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 54.1534\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 53.7590\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 53.3673\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 52.9785\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 52.5925\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 52.2094\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 51.8289\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 51.4513\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 51.0765\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 50.7045\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 50.3351\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 49.9683\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 49.6043\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 49.2428\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 48.8841\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 48.5279\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 48.1744\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 47.8235\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 47.4749\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 47.1292\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 46.7858\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 46.4449\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 46.1066\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 45.7706\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 45.4371\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 45.1061\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 44.7775\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 44.4512\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 44.1273\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 43.8058\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 43.4868\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 43.1699\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 42.8554\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 42.5432\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 42.2332\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 41.9256\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 41.6201\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 41.3169\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 41.0159\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 40.7170\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 40.4204\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 40.1259\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 39.8335\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 39.5434\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 39.2553\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 38.9693\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 38.6854\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 38.4035\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 38.1237\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.8459\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 37.5702\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.2965\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 37.0247\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.7550\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 36.4873\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 36.2214\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 35.9575\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 35.6956\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 35.4355\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 35.1773\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 34.9210\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 34.6666\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 34.4140\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 34.1633\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 33.9144\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 33.6673\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 33.4220\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 33.1785\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 32.9368\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 32.6968\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 32.4587\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 32.2222\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 31.9874\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 31.7544\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 31.5230\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 31.2934\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 31.0654\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 30.8391\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 30.6143\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 30.3913\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 30.1699\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 29.9501\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 29.7319\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 29.5153\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 29.3003\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 29.0868\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28.8749\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 28.6645\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28.4557\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 28.2483\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 28.0425\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 27.8383\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 27.6354\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 27.4341\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 27.2342\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 27.0358\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 26.8388\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 26.6433\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 26.4492\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 26.2565\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 26.0652\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 25.8753\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 25.6868\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 25.4996\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 25.3138\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 25.1294\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.9463\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.7645\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.5841\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.4051\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.2272\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 24.0507\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 23.8755\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 23.7016\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 4ms/step - loss: 23.5289\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 23.3575\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 23.1873\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 23.0184\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.8507\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.6842\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.5189\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.3549\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.1920\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.0303\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 21.8698\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 21.7105\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 21.5523\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 21.3953\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 21.2394\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 21.0847\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 20.9310\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 20.7786\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 20.6271\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 20.4769\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 20.3277\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 20.1796\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 20.0326\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.8866\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.7417\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.5979\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.4552\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.3134\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 19.1727\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 19.0330\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 18.8944\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 18.7567\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 18.6201\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 18.4844\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.3497\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.2161\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 18.0833\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.9515\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.8208\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.6909\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 17.5621\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 17.4341\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 17.3071\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 17.1810\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 256ms/step - loss: 17.0558\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.9315\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 16.8082\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 16.6858\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16.5641\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16.4435\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 16.3237\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16.2048\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 16.0867\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.9695\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.8531\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.7377\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 15.6230\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.5092\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 15.3962\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.2841\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 15.1726\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 15.0621\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 14.9524\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 14.8435\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.7353\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 14.6280\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.5214\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 14.4156\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.3106\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 14.2063\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 14.1028\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 14.0001\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.8981\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.7968\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.6963\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.5965\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.4974\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.3991\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 13.3015\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 13.2046\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 13.1084\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 13.0129\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 12.9181\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.8240\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.7305\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 12.6378\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 12.5457\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 12.4543\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 12.3636\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.2735\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 12.1841\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 12.0953\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.0072\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 11.9197\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 11.8328\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.7467\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 11.6610\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 11.5761\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.4918\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.4080\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.3250\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 11.2425\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 11.1606\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 11.0792\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.9985\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.9184\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 10.8388\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 10.7599\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 10.6815\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 10.6037\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.5264\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.4497\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.3736\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.2980\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.2229\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.1485\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 10.0745\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 10.0012\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.9283\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.8560\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.7841\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.7128\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.6421\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.5718\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.5021\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.4329\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.3641\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 90ms/step - loss: 9.2959\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.2282\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.1610\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.0942\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.0279\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.9622\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.8969\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.8321\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.7677\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.7039\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.6404\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.5775\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.5150\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.4529\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.3914\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.3302\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.2696\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.2093\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.1495\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.0901\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.0312\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.9727\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.9146\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.8569\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7997\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7429\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.6864\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.6304\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5748\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.5197\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.4649\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4105\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.3565\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.3029\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.2497\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.1969\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.1444\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.0924\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.0407\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.9894\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.9385\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8879\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8378\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.7879\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 6.7385\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6894\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.6407\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.5923\n",
            "[403.70343]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Excerise"
      ],
      "metadata": {
        "id": "HIKpPJE8cZQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load fashion dataset\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZDBjcahdAa7",
        "outputId": "924b2e1f-283d-46c3-bf13-eaa8f45a311e"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m0QuSPZwmGn-",
        "outputId": "06fa0225-bf9e-45c5-d481-2a9c899ee67c"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_images.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LlDk8cJdmKdh",
        "outputId": "97cd623f-2a62-4436-9626-c72f46072d38"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 28, 28)"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "plt.imshow(training_images[0])\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 780
        },
        "id": "7SvhecN2gLPs",
        "outputId": "4c16e527-1d55-4052-e85f-75f6f955a1ba"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize data \n",
        "training_images = training_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "id": "-_ibD6PcgyEv"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.cbook import flatten\n",
        "#Design model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                   tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                                   tf.keras.layers.Dense(10, activation=\"softmax\")])"
      ],
      "metadata": {
        "id": "xLdSHsRkg95m"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss= \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "model.fit(training_images, training_labels, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWUODQIoifEG",
        "outputId": "24521aab-5e4e-49e8-a9b0-a934a04bec58"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.4904 - accuracy: 0.8264\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3764 - accuracy: 0.8638\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3352 - accuracy: 0.8771\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3091 - accuracy: 0.8867\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2931 - accuracy: 0.8924\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fc6adc71b10>"
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0UV159Qk1c5",
        "outputId": "f78e9a09-1198-4316-f802-60bd8b0b45f3"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 1ms/step - loss: 0.3665 - accuracy: 0.8680\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3665219843387604, 0.8679999709129333]"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(tf.math.round(classifications[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-mxud3VlZzR",
        "outputId": "fa9244e1-a2bc-4f0b-90d6-6f60307104f5"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(5, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction [9,2,1,1,6]\n",
        "test_labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJguULpnu1R",
        "outputId": "532a97e1-4317-4eb4-f1bc-bb8d4585b53d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.argmax(classifications[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wjVSWjZpg5tA",
        "outputId": "b18c27e9-611b-4ea4-ffc4-04b555e52ef0"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "9"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Outmate classification validation for a wanted number of examples \n",
        "def output_checker(number_of_examples):\n",
        "  right = []\n",
        "  wrong = []\n",
        "  for i in number_of_examples:\n",
        "    if np.argmax(classifications[i]) == test_labels[i]:\n",
        "      #print(f'{i} prediction is right')\n",
        "      right.append(i)\n",
        "      \n",
        "\n",
        "    else:\n",
        "      #print(f'{i} prediction is wrong')\n",
        "      wrong.append(i)\n",
        "\n",
        "  #print(right)\n",
        "  print(f'{len(wrong)} of {number_of_examples} are wrong classified \\n List of wrong classified pictures: \\n {wrong}')"
      ],
      "metadata": {
        "id": "k_Jyj09dh_0a"
      },
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_checker(range(1000))\n",
        "output_checker"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9bTH5M4ljJsc",
        "outputId": "98bb6cc2-87de-4330-9d2d-67d4d3708cc3"
      },
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "122 of range(0, 1000) are wrong classified \n",
            " List of wrong classified pictures: \n",
            " [12, 17, 23, 25, 40, 42, 43, 49, 51, 66, 68, 74, 89, 103, 107, 135, 147, 150, 151, 192, 219, 222, 227, 239, 241, 244, 249, 255, 271, 282, 285, 286, 289, 309, 316, 324, 325, 332, 344, 359, 361, 367, 378, 381, 382, 396, 406, 409, 413, 441, 444, 454, 457, 460, 474, 476, 490, 491, 511, 512, 527, 529, 546, 548, 562, 563, 565, 569, 572, 578, 581, 586, 587, 595, 601, 608, 632, 634, 635, 639, 640, 663, 669, 670, 671, 686, 688, 701, 711, 717, 722, 725, 732, 750, 753, 760, 761, 787, 793, 800, 801, 823, 851, 890, 898, 902, 905, 921, 926, 930, 935, 956, 960, 963, 965, 966, 968, 976, 977, 979, 985, 994]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.output_checker(number_of_examples)>"
            ]
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Ex"
      ],
      "metadata": {
        "id": "xEdJlqsKCBKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now test with 512 neurons\n",
        "#What different results do you get for loss, training time etc? \n",
        "#Why do you think that's the case?\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viOkXZfpCEB-",
        "outputId": "e70c9cc9-024c-45a2-accf-2c390ddab8e5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4771 - accuracy: 0.8302\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3599 - accuracy: 0.8686\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3221 - accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2984 - accuracy: 0.8897\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2814 - accuracy: 0.8962\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3381 - accuracy: 0.8781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy8a_RuMH1ES",
        "outputId": "7ff2e03f-6e65-45d4-a159-10f0a0f5fece"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hypothesis: Double the amount of neurons will increase accuracy\n",
        "#But processing will take more time \n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdwifcFcINAC",
        "outputId": "0b7a94b6-8a69-4bf2-d5e7-6225544436bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.4712 - accuracy: 0.8321\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.3583 - accuracy: 0.8682\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.3226 - accuracy: 0.8813\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2969 - accuracy: 0.8901\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2816 - accuracy: 0.8956\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3326 - accuracy: 0.8821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result: Training takes longer and accuracy is increased from 0.8656 to 0.8757\n",
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrfGX58nJXaF",
        "outputId": "88622ff7-62fc-4a54-c772-57693916a295"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Ex"
      ],
      "metadata": {
        "id": "5TM3gP1YCKlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What would happen if you remove the Flatten() layer. Why do you think that's the case?\n",
        "\n",
        "#=> Error: Because shape of data. \n",
        "#RoT = First Layer should be in same shape as input data \n",
        "#With Flatten we create of out 28x28 matrix an array of 784X1\n"
      ],
      "metadata": {
        "id": "xLCWk4JyCPCc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Ex"
      ],
      "metadata": {
        "id": "nESok22sCONn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the final (output) layers. Why are there 10 of them? \n",
        "#What would happen if you had a different amount than 10? For example, try training the network with 5.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "PJb3ydI7CTPa"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 Ex"
      ],
      "metadata": {
        "id": "0ddP6xKNCUDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the effects of additional layers in the network. \n",
        "#What will happen if you add another layer between the one with 512 and the final layer with 10.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "_8gw2Jp9CbDV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 Ex"
      ],
      "metadata": {
        "id": "tJ3meIPJCYyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the impact of training for more or less epochs. Why do you think that would be the case?\n",
        "\n",
        "#=>\n"
      ],
      "metadata": {
        "id": "BDs6n82hCYT6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Ex"
      ],
      "metadata": {
        "id": "QCWoo1T5XTd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. \n",
        "#What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?\n",
        "\n",
        "#=>\n",
        "\n"
      ],
      "metadata": {
        "id": "5sOEqevvXWeL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Ex"
      ],
      "metadata": {
        "id": "Xpa-faKNXjIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier when you trained for extra epochs you had an issue where your loss might change. It might have taken a bit of time for you to wait for the training to do that, and you might have thought 'wouldn't it be nice if I could stop the training when I reach a desired value?' -- i.e. 95% accuracy might be enough for you, and if you reach that after 3 epochs, why sit around waiting for it to finish a lot more epochs....So how would you fix that? Like any other program...you have callbacks! Let's see them in action..."
      ],
      "metadata": {
        "id": "o1cHN4d1XwRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get(\"acc\") > 0.9):\n",
        "      print(\"\\nReached 85% of accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "  \n",
        "callbacks = myCallback()\n",
        "\n",
        "#Create Dataset\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images = training_images/255\n",
        "test_images = test_images/255\n",
        "\n",
        "#Create model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"acc\")\n",
        "model.fit(training_images, training_labels, epochs=8, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFuOEyMbxhTs",
        "outputId": "be072141-9ac6-49ef-ab0b-6b1653e03294"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4723 - acc: 0.8309\n",
            "Epoch 2/8\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3558 - acc: 0.8694\n",
            "Epoch 3/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3221 - acc: 0.8808\n",
            "Epoch 4/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2991 - acc: 0.8889\n",
            "Epoch 5/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2798 - acc: 0.8952\n",
            "Epoch 6/8\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9011\n",
            "Reached 85% of accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2651 - acc: 0.9012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46c6403750>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance evaluation\n",
        "model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)\n",
        "print(tf.math.round(predictions[:5]))\n",
        "print(test_labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plem09ri5jn1",
        "outputId": "12f10e12-9a22-4d58-86bb-b8a8f41b12e8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3349 - acc: 0.8810\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(5, 10), dtype=float32)\n",
            "[9 2 1 1 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANzb1_rm9nam",
        "outputId": "cf03ef13-7cd2-41d0-8407-92c122da00af"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_17\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " flatten_15 (Flatten)        (None, 784)               0         \n",
            "                                                                 \n",
            " dense_30 (Dense)            (None, 512)               401920    \n",
            "                                                                 \n",
            " dense_31 (Dense)            (None, 10)                5130      \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 407,050\n",
            "Trainable params: 407,050\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2: TF.ComputerVision (CV)"
      ],
      "metadata": {
        "id": "j3GUyO_Df9l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKHDSeb8gBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 3: TF.Natrual Language Processing (NLP)"
      ],
      "metadata": {
        "id": "YsyqqzzVgCdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yGrzdqAjgIYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 4: TF.Time Series Forcasting"
      ],
      "metadata": {
        "id": "1AnDmEu3gMew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MKNNViULgUSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}