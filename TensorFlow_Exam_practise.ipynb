{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TensorFlow - Exam practise.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "ueEq3n3_gd1W",
        "YShLWZuA0RIv",
        "LRoqLzxB9gU9",
        "xEdJlqsKCBKt",
        "5TM3gP1YCKlE",
        "nESok22sCONn",
        "0ddP6xKNCUDh",
        "tJ3meIPJCYyZ",
        "QCWoo1T5XTd6",
        "Xpa-faKNXjIo",
        "1AnDmEu3gMew"
      ],
      "authorship_tag": "ABX9TyOhjIggHV+/kjU0qbY3f0Qu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/reneSalmon/Google_TensorFlow_Certificate/blob/master/TensorFlow_Exam_practise.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 1: TF.Basics\n"
      ],
      "metadata": {
        "id": "9LEN1H22f2zs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1 Quickstart for Beginners"
      ],
      "metadata": {
        "id": "ueEq3n3_gd1W"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load a prebuild dataset\n",
        "2. Build neural network that classifies images\n",
        "3. Train neural network\n",
        "4. Evaluat accuracy of model"
      ],
      "metadata": {
        "id": "AjGO40BLhEam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Setup\n",
        "import tensorflow as tf\n",
        "print(\"TensorFlow version:\", tf.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QfCGSB4ahD98",
        "outputId": "0af857b8-997d-4572-dc42-62d1790cbdb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TensorFlow version: 2.8.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTgTW33HfwCM",
        "outputId": "ba446d43-55ca-496b-b9e5-415eb3e61841"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# 1 Load dataset\n",
        "minst = tf.keras.datasets.mnist #load dataset of 70.000 handwriting digits for image classification\n",
        "(x_train, y_train), (x_test, y_test) = minst.load_data() #split dataset x = images, y = image labels\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0 #normalize dataset"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "import numpy as np\n",
        "np.set_printoptions(linewidth=200)\n",
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(x_train[0])\n",
        "print(y_train[0])\n",
        "print(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "7rww5zcfeHpb",
        "outputId": "dd13e273-8a1f-4475-8aa2-ff708addee59"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "5\n",
            "[[0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.01176471 0.07058824 0.07058824 0.07058824 0.49411765\n",
            "  0.53333333 0.68627451 0.10196078 0.65098039 1.         0.96862745 0.49803922 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.11764706 0.14117647 0.36862745 0.60392157 0.66666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.99215686 0.88235294 0.6745098  0.99215686 0.94901961 0.76470588 0.25098039 0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.19215686 0.93333333 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.98431373 0.36470588 0.32156863 0.32156863 0.21960784 0.15294118 0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.07058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.71372549 0.96862745\n",
            "  0.94509804 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.31372549 0.61176471 0.41960784 0.99215686 0.99215686 0.80392157 0.04313725 0.         0.16862745\n",
            "  0.60392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.05490196 0.00392157 0.60392157 0.99215686 0.35294118 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.54509804 0.99215686 0.74509804 0.00784314 0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.04313725 0.74509804 0.99215686 0.2745098  0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.1372549  0.94509804 0.88235294 0.62745098 0.42352941\n",
            "  0.00392157 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.31764706 0.94117647 0.99215686 0.99215686\n",
            "  0.46666667 0.09803922 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.17647059 0.72941176 0.99215686\n",
            "  0.99215686 0.58823529 0.10588235 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.0627451  0.36470588\n",
            "  0.98823529 0.99215686 0.73333333 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.97647059 0.99215686 0.97647059 0.25098039 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.18039216 0.50980392 0.71764706\n",
            "  0.99215686 0.99215686 0.81176471 0.00784314 0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.15294118 0.58039216 0.89803922 0.99215686 0.99215686\n",
            "  0.99215686 0.98039216 0.71372549 0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.09411765 0.44705882 0.86666667 0.99215686 0.99215686 0.99215686 0.99215686\n",
            "  0.78823529 0.30588235 0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.09019608 0.25882353 0.83529412 0.99215686 0.99215686 0.99215686 0.99215686 0.77647059 0.31764706\n",
            "  0.00784314 0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.07058824 0.67058824 0.85882353 0.99215686 0.99215686 0.99215686 0.99215686 0.76470588 0.31372549 0.03529412 0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.21568627 0.6745098  0.88627451 0.99215686 0.99215686 0.99215686 0.99215686 0.95686275 0.52156863 0.04313725 0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.53333333 0.99215686 0.99215686 0.99215686 0.83137255 0.52941176 0.51764706 0.0627451  0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]\n",
            " [0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.\n",
            "  0.         0.         0.         0.         0.         0.         0.         0.         0.         0.         0.        ]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Build machine learning model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
        "    tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "    tf.keras.layers.Dropout(0.2),\n",
        "    tf.keras.layers.Dense(10)\n",
        "])"
      ],
      "metadata": {
        "id": "_Q5RlFIvk3iB"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 For each example, the model returns a vector of logits or log-odds scores, one for each class.\n",
        "predictions = model(x_train[:1]).numpy()\n",
        "predictions"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TM7eI3FKnWlf",
        "outputId": "624589a1-968f-4f90-d6f1-0f8fa9664b08"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[-0.08420402,  0.05257533,  0.7750828 ,  0.13177018, -0.36723143,  0.34245336,  0.04734165,  0.18702422, -0.55256885,  0.01669562]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 The tf.nn.softmax function converts these logits to probabilities for each class\n",
        "tf.nn.softmax(predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwzAyoC7n3GV",
        "outputId": "3be8b37e-3f03-4d3f-d4c5-3b64b988cdc7"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.08183906, 0.09383464, 0.19326098, 0.10156804, 0.06166567, 0.12538789, 0.09334482, 0.10733802, 0.0512333 , 0.09052756]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5 Define a loss function for training using losses.SparseCategoricalCrossentropy,\n",
        "# which takes a vector of logits and a True index and returns a scalar loss for each example.\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)"
      ],
      "metadata": {
        "id": "ugtsy4a8oagx"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6 This loss is equal to the negative log probability of the true class: The loss is zero if the model is sure of the correct class.\n",
        "# This untrained model gives probabilities close to random (1/10 for each class), \n",
        "# so the initial loss should be close to -tf.math.log(1/10) ~= 2.3.\n",
        "loss_fn(y_train[:1], predictions).numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X5G-X4KWp-zT",
        "outputId": "b76dec11-481b-4cd8-8d99-f2c7bd6a2cf9"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2.0763433"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7 Configure and compile the model \n",
        "model.compile(optimizer=\"adam\",\n",
        "              loss=loss_fn,\n",
        "              metrics=[\"accuracy\"])"
      ],
      "metadata": {
        "id": "EWzXu-OcqjcW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train and evaluate the model\n"
      ],
      "metadata": {
        "id": "73obEdgPrONt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Adjust model parameters and minimize loss\n",
        "model.fit(x_train, y_train, epochs=5)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MZh7dggFrVcW",
        "outputId": "db8e2783-f86b-4052-8db5-970e22fd06d6"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2896 - accuracy: 0.9166\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1397 - accuracy: 0.9588\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.1047 - accuracy: 0.9690\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 6s 3ms/step - loss: 0.0861 - accuracy: 0.9729\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0733 - accuracy: 0.9772\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46d860e310>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Check the model performance \n",
        "model.evaluate(x_test, y_test, verbose=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k_eP2je8nD3E",
        "outputId": "f9619373-172f-4fb8-bb2a-f6419070006a"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 - 1s - loss: 0.0706 - accuracy: 0.9780 - 743ms/epoch - 2ms/step\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.07062876224517822, 0.9779999852180481]"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# => Image classifier accuracy ~98% on this dataset"
      ],
      "metadata": {
        "id": "Q_hsgXqQtFQQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Return probability of the matching for a specific test\n",
        "probability_model = tf.keras.Sequential([\n",
        "    model, \n",
        "    tf.keras.layers.Softmax()\n",
        "])"
      ],
      "metadata": {
        "id": "_YVodfq3ukGH"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = probability_model(x_test[:5])\n",
        "results"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQKL1COGvpkZ",
        "outputId": "90d6188f-b597-4909-8914-61cf88d1789b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[1.05880503e-07, 3.14974038e-08, 1.11400905e-05, 1.59035641e-04, 2.12891615e-09, 1.36830170e-06, 3.07337245e-13, 9.99823034e-01, 1.72061186e-06, 3.67776920e-06],\n",
              "       [9.80112169e-09, 1.78456743e-04, 9.99809444e-01, 1.18457783e-05, 7.37922039e-13, 2.35813133e-07, 4.98748385e-08, 7.38390401e-14, 1.45737715e-08, 3.61749747e-14],\n",
              "       [2.97374697e-07, 9.98688281e-01, 4.40564763e-05, 6.02909540e-06, 7.85946831e-05, 3.28993679e-07, 6.94351093e-06, 1.10241654e-03, 7.24806014e-05, 4.58989888e-07],\n",
              "       [9.99060333e-01, 1.59543099e-08, 5.42044290e-04, 1.40105965e-06, 1.07855185e-05, 4.26439692e-05, 3.04801826e-04, 6.22704192e-06, 8.29306259e-08, 3.16035657e-05],\n",
              "       [3.75721447e-06, 1.36673661e-09, 2.80348604e-06, 4.09558876e-09, 9.98954415e-01, 1.08862662e-06, 6.01494976e-05, 3.00818774e-05, 1.24983887e-06, 9.46251093e-04]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rounded_resultst = tf.math.round(results)\n",
        "rounded_resultst"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kVOwPax37rNu",
        "outputId": "97d1fbce-415e-4eb0-ff6e-1218d4ebb3b2"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(5, 10), dtype=float32, numpy=\n",
              "array([[0., 0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
              "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
              "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Suggestion for results = [7, 2, 1, 0, 4]\n",
        "# How to show the results in numbers automatically?\n",
        "# How to compare the results?\n",
        "y_test[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BaEPvBWwJCZ",
        "outputId": "c621c2c9-87b1-4b2c-fd0c-3bd7caf0329b"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([7, 2, 1, 0, 4], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2 Exercise"
      ],
      "metadata": {
        "id": "YShLWZuA0RIv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow import keras \n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "xpoKNpIE1MDk"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1 Define and compile neural network\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])]) # 1 layer / 1 neuron"
      ],
      "metadata": {
        "id": "0Nx5gKSO0nyn"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2 Compile NN with 2 functions: Loss and optimizer\n",
        "model.compile(optimizer=\"sgd\", loss=\"mean_squared_error\")"
      ],
      "metadata": {
        "id": "__ckCS-r1T9d"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3 Providing data\n",
        "xs = np.array([-1.0,  0.0, 1.0, 2.0, 3.0, 4.0], dtype=float)\n",
        "ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float)"
      ],
      "metadata": {
        "id": "q_M7Hoh-4Ve-"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4 Training NN\n",
        "model.fit(xs, ys, epochs=500)\n",
        "print(model.predict([10.0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NQzzq7kJ4tA-",
        "outputId": "86432016-fa47-4153-fa96-b261be4b1c47"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 0s 288ms/step - loss: 35.2771\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28.0898\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 22.4283\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 17.9674\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 14.4511\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 11.6782\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.4903\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.7627\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.3975\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.3174\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4618\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7829\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2433\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8132\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.4694\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.1937\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.9717\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7919\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.6455\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5255\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 1.4264\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.3437\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2742\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2150\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1640\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1197\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0806\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.0457\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.0143\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9856\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9592\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9347\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.9117\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8899\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.8693\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.8496\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8307\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.8125\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7949\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.7778\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7613\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.7452\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.7296\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.7143\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6994\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6849\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6707\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.6568\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.6432\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6300\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.6170\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.6043\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5918\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.5796\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5677\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.5560\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.5446\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.5334\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5224\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.5117\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.5012\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4909\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4808\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4709\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.4612\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4518\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4425\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4334\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4245\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.4158\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.4072\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3989\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3907\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3827\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3748\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.3671\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3596\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3522\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.3449\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.3378\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3309\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3241\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3175\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.3109\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.3045\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2983\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.2922\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2862\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2803\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2745\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 0.2689\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.2634\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2580\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2527\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2475\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2424\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2374\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2325\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2278\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.2231\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.2185\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.2140\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.2096\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.2053\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.2011\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1970\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1929\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1889\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1851\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1813\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1775\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1739\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.1703\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1668\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1634\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1600\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.1568\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1535\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1504\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1473\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1443\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1413\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1384\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1356\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.1328\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1300\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1274\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1248\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.1222\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1197\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1172\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1148\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1125\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1102\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1079\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1057\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1035\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.1014\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0993\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0973\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0953\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0933\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0914\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0895\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 0.0877\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0859\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0841\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0824\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0807\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0790\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0774\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0758\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0743\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0727\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0712\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0698\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0683\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0669\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0656\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0642\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0629\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0616\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0603\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0591\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0579\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0567\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0555\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0544\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0533\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0522\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0511\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0501\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0490\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0480\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0470\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0461\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0451\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0442\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0433\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0424\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0415\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0407\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0398\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0390\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0382\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 319ms/step - loss: 0.0374\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0367\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0359\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0352\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0345\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0337\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 21ms/step - loss: 0.0331\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 0.0324\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0317\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0311\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0304\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0298\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0292\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0286\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0280\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0274\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0269\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0263\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0258\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0252\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0247\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0242\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0237\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0232\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0227\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0223\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0218\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0214\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0209\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0205\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0201\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 0.0197\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 0.0193\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0189\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0185\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0181\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0177\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0174\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0170\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0167\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0163\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0160\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0157\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0153\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0150\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0147\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0144\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0141\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0138\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0135\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0133\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0130\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0127\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0125\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0122\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0120\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0117\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0115\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0112\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0110\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0108\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0106\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0103\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0101\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0099\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0097\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0095\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0093\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0091\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0089\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0088\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0086\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0084\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0082\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0081\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0079\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0077\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0076\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0074\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0073\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0071\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0070\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0068\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0067\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0065\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0064\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0063\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0062\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0060\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0059\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0058\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0057\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0055\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0054\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0053\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0052\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0051\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0050\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0049\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0048\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0047\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0046\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0045\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0044\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0043\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0042\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0041\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0040\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0039\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0038\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.0037\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0037\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 0.0036\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0035\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0034\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0034\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0033\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0032\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0032\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0031\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0030\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0029\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0029\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0028\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0027\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0027\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 0.0026\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0026\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0025\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0025\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0024\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0023\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0022\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0021\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0020\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0020\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0019\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0019\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0018\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0018\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0017\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0016\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0016\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0015\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0014\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0014\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0014\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0013\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 0.0013\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 0.0012\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 0.0012\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 308ms/step - loss: 0.0012\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0011\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.0011\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0011\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 0.0011\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.0010\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.0010\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.9063e-04\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.7028e-04\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.5035e-04\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 9.3083e-04\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 9.1171e-04\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.9298e-04\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 8.7464e-04\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 8.5667e-04\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.3908e-04\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 8.2184e-04\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.0496e-04\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.8843e-04\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.7223e-04\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.5637e-04\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.4083e-04\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2561e-04\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.1071e-04\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.9611e-04\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8181e-04\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6781e-04\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5409e-04\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.4066e-04\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.2750e-04\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.1461e-04\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.0199e-04\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.8962e-04\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.7751e-04\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.6565e-04\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.5403e-04\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.4265e-04\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.3150e-04\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 5.2058e-04\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.0989e-04\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.9942e-04\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.8916e-04\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.7911e-04\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.6927e-04\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.5963e-04\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.5019e-04\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 4.4094e-04\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.3188e-04\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 4.2301e-04\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.1433e-04\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 4.0582e-04\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.9748e-04\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8931e-04\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.8132e-04\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.7349e-04\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.6581e-04\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5830e-04\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.5094e-04\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.4373e-04\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.3667e-04\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 3.2976e-04\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.2298e-04\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.1635e-04\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0985e-04\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 3.0349e-04\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9725e-04\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.9115e-04\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.8517e-04\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7931e-04\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.7357e-04\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6795e-04\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.6245e-04\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.5706e-04\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.5178e-04\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.4661e-04\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.4154e-04\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3658e-04\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.3172e-04\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.2696e-04\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.2230e-04\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1773e-04\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2.1326e-04\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 285ms/step - loss: 2.0888e-04\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.0459e-04\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 2.0039e-04\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9627e-04\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.9224e-04\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.8829e-04\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.8442e-04\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.8064e-04\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7692e-04\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.7329e-04\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6973e-04\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6624e-04\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.6283e-04\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5948e-04\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.5621e-04\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.5300e-04\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4986e-04\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.4678e-04\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 1.4376e-04\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 1.4081e-04\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1.3792e-04\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3509e-04\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 1.3231e-04\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 1.2959e-04\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.2693e-04\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2433e-04\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.2177e-04\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1927e-04\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.1682e-04\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1442e-04\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.1207e-04\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0977e-04\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0751e-04\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 1.0530e-04\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 1.0314e-04\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 1.0102e-04\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.8946e-05\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.6913e-05\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 9.4924e-05\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.2974e-05\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 9.1065e-05\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.9195e-05\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 22ms/step - loss: 8.7363e-05\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.5567e-05\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 8.3811e-05\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 8.2088e-05\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0403e-05\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.8752e-05\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.7135e-05\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 7.5550e-05\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 7.3998e-05\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 7.2478e-05\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.0990e-05\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.9530e-05\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.8102e-05\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6.6704e-05\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 6.5334e-05\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.3992e-05\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 6.2678e-05\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 6.1390e-05\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 6.0129e-05\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 5.8895e-05\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 5.7685e-05\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 5.6499e-05\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 5.5338e-05\n",
            "[[18.978296]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3 Exercise"
      ],
      "metadata": {
        "id": "LRoqLzxB9gU9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise you'll try to build a neural network that predicts the price of a house according to a simple formula.\n",
        "\n",
        "So, imagine if house pricing was as easy as a house costs 50k + 50k per bedroom, so that a 1 bedroom house costs 100k, a 2 bedroom house costs 150k etc.\n",
        "\n",
        "How would you create a neural network that learns this relationship so that it would predict a 7 bedroom house as costing close to 400k etc.\n",
        "\n",
        "Hint: Your network might work better if you scale the house price down. You don't have to give the answer 400...it might be better to create something that predicts the number 4, and then your answer is in the 'hundreds of thousands' etc."
      ],
      "metadata": {
        "id": "-hfRrFTf936S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: house_model\n",
        "def house_model(y_new):\n",
        "    x = np.array([1.0, 2.0, 3.0, 4.0, 5.0 , 6.0], dtype=float)\n",
        "    y = np.array([100.0, 150.0, 200.0, 250.0, 300.0, 350.0], dtype=float)\n",
        "    model = keras.Sequential([keras.layers.Dense(units=1, input_shape=[1])])\n",
        "    model.compile(optimizer=\"SGD\", loss=\"mse\")\n",
        "    model.fit(x,y, epochs=500)\n",
        "    return model.predict(y_new)[0]"
      ],
      "metadata": {
        "id": "fcmFakXB9on3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prediction = house_model([7])\n",
        "print(prediction)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixnrSD1VCYt-",
        "outputId": "80b6606a-88f5-454d-d60f-2a936aa65bf4"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/500\n",
            "1/1 [==============================] - 1s 707ms/step - loss: 59849.0664\n",
            "Epoch 2/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 27832.9043\n",
            "Epoch 3/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 13013.8955\n",
            "Epoch 4/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 6154.2544\n",
            "Epoch 5/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 2978.4514\n",
            "Epoch 6/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 1507.6461\n",
            "Epoch 7/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 825.9727\n",
            "Epoch 8/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 509.5399\n",
            "Epoch 9/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 362.1578\n",
            "Epoch 10/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 293.0248\n",
            "Epoch 11/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 260.1143\n",
            "Epoch 12/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 243.9752\n",
            "Epoch 13/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 235.6054\n",
            "Epoch 14/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 230.8377\n",
            "Epoch 15/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 227.7437\n",
            "Epoch 16/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 225.4306\n",
            "Epoch 17/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 223.4858\n",
            "Epoch 18/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 221.7176\n",
            "Epoch 19/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 220.0374\n",
            "Epoch 20/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 218.4043\n",
            "Epoch 21/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 216.7993\n",
            "Epoch 22/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 215.2135\n",
            "Epoch 23/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 213.6425\n",
            "Epoch 24/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 212.0846\n",
            "Epoch 25/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 210.5389\n",
            "Epoch 26/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 209.0046\n",
            "Epoch 27/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 207.4818\n",
            "Epoch 28/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 205.9702\n",
            "Epoch 29/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 204.4695\n",
            "Epoch 30/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 202.9799\n",
            "Epoch 31/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 201.5011\n",
            "Epoch 32/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 200.0329\n",
            "Epoch 33/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 198.5755\n",
            "Epoch 34/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 197.1290\n",
            "Epoch 35/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 195.6926\n",
            "Epoch 36/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 194.2668\n",
            "Epoch 37/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 192.8516\n",
            "Epoch 38/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 191.4465\n",
            "Epoch 39/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 190.0517\n",
            "Epoch 40/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 188.6670\n",
            "Epoch 41/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 187.2925\n",
            "Epoch 42/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 185.9280\n",
            "Epoch 43/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 184.5735\n",
            "Epoch 44/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 183.2288\n",
            "Epoch 45/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 181.8939\n",
            "Epoch 46/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 180.5685\n",
            "Epoch 47/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 179.2532\n",
            "Epoch 48/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 177.9471\n",
            "Epoch 49/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 176.6507\n",
            "Epoch 50/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 175.3636\n",
            "Epoch 51/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 174.0859\n",
            "Epoch 52/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 172.8177\n",
            "Epoch 53/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 171.5587\n",
            "Epoch 54/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 170.3088\n",
            "Epoch 55/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 169.0679\n",
            "Epoch 56/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 167.8362\n",
            "Epoch 57/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 166.6135\n",
            "Epoch 58/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 165.3994\n",
            "Epoch 59/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 164.1945\n",
            "Epoch 60/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 162.9983\n",
            "Epoch 61/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 161.8107\n",
            "Epoch 62/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 160.6319\n",
            "Epoch 63/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 159.4615\n",
            "Epoch 64/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 158.2997\n",
            "Epoch 65/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 157.1465\n",
            "Epoch 66/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 156.0016\n",
            "Epoch 67/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 154.8650\n",
            "Epoch 68/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 153.7368\n",
            "Epoch 69/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 152.6167\n",
            "Epoch 70/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 151.5048\n",
            "Epoch 71/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 150.4010\n",
            "Epoch 72/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 149.3053\n",
            "Epoch 73/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 148.2175\n",
            "Epoch 74/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 147.1376\n",
            "Epoch 75/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 146.0655\n",
            "Epoch 76/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 145.0014\n",
            "Epoch 77/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 143.9450\n",
            "Epoch 78/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 142.8963\n",
            "Epoch 79/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 141.8553\n",
            "Epoch 80/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 140.8217\n",
            "Epoch 81/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 139.7958\n",
            "Epoch 82/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 138.7773\n",
            "Epoch 83/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 137.7663\n",
            "Epoch 84/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 136.7626\n",
            "Epoch 85/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 135.7661\n",
            "Epoch 86/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 134.7770\n",
            "Epoch 87/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 133.7952\n",
            "Epoch 88/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 132.8204\n",
            "Epoch 89/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 131.8527\n",
            "Epoch 90/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 130.8920\n",
            "Epoch 91/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 129.9385\n",
            "Epoch 92/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 128.9917\n",
            "Epoch 93/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 128.0520\n",
            "Epoch 94/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 127.1190\n",
            "Epoch 95/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 126.1929\n",
            "Epoch 96/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 125.2736\n",
            "Epoch 97/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 124.3609\n",
            "Epoch 98/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 123.4548\n",
            "Epoch 99/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 122.5554\n",
            "Epoch 100/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 121.6625\n",
            "Epoch 101/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 120.7761\n",
            "Epoch 102/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 119.8962\n",
            "Epoch 103/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 119.0227\n",
            "Epoch 104/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 118.1555\n",
            "Epoch 105/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 117.2946\n",
            "Epoch 106/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 116.4401\n",
            "Epoch 107/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 115.5918\n",
            "Epoch 108/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 114.7495\n",
            "Epoch 109/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 113.9136\n",
            "Epoch 110/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 113.0837\n",
            "Epoch 111/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 112.2598\n",
            "Epoch 112/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 111.4419\n",
            "Epoch 113/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 110.6300\n",
            "Epoch 114/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 109.8240\n",
            "Epoch 115/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 109.0239\n",
            "Epoch 116/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 108.2296\n",
            "Epoch 117/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 107.4411\n",
            "Epoch 118/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 106.6583\n",
            "Epoch 119/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 105.8813\n",
            "Epoch 120/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 105.1098\n",
            "Epoch 121/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 104.3441\n",
            "Epoch 122/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 103.5839\n",
            "Epoch 123/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 102.8293\n",
            "Epoch 124/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 102.0800\n",
            "Epoch 125/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 101.3364\n",
            "Epoch 126/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 100.5981\n",
            "Epoch 127/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 99.8651\n",
            "Epoch 128/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 99.1376\n",
            "Epoch 129/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 98.4153\n",
            "Epoch 130/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 97.6982\n",
            "Epoch 131/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 96.9864\n",
            "Epoch 132/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 96.2798\n",
            "Epoch 133/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 95.5784\n",
            "Epoch 134/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 94.8821\n",
            "Epoch 135/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 94.1908\n",
            "Epoch 136/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 93.5046\n",
            "Epoch 137/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 92.8234\n",
            "Epoch 138/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 92.1471\n",
            "Epoch 139/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 91.4757\n",
            "Epoch 140/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 90.8092\n",
            "Epoch 141/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 90.1477\n",
            "Epoch 142/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 89.4909\n",
            "Epoch 143/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 88.8389\n",
            "Epoch 144/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 88.1917\n",
            "Epoch 145/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 87.5492\n",
            "Epoch 146/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 86.9114\n",
            "Epoch 147/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 86.2782\n",
            "Epoch 148/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 85.6495\n",
            "Epoch 149/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 85.0255\n",
            "Epoch 150/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 84.4061\n",
            "Epoch 151/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 83.7912\n",
            "Epoch 152/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 83.1806\n",
            "Epoch 153/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 82.5747\n",
            "Epoch 154/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 81.9730\n",
            "Epoch 155/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 81.3758\n",
            "Epoch 156/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 80.7829\n",
            "Epoch 157/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 80.1944\n",
            "Epoch 158/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 79.6101\n",
            "Epoch 159/500\n",
            "1/1 [==============================] - 0s 19ms/step - loss: 79.0302\n",
            "Epoch 160/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 78.4544\n",
            "Epoch 161/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 77.8828\n",
            "Epoch 162/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 77.3154\n",
            "Epoch 163/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 76.7521\n",
            "Epoch 164/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 76.1928\n",
            "Epoch 165/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 75.6378\n",
            "Epoch 166/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 75.0866\n",
            "Epoch 167/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 74.5397\n",
            "Epoch 168/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 73.9965\n",
            "Epoch 169/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 73.4574\n",
            "Epoch 170/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 72.9223\n",
            "Epoch 171/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 72.3910\n",
            "Epoch 172/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 71.8636\n",
            "Epoch 173/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 71.3401\n",
            "Epoch 174/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 70.8203\n",
            "Epoch 175/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 70.3043\n",
            "Epoch 176/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 69.7922\n",
            "Epoch 177/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 69.2836\n",
            "Epoch 178/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 68.7789\n",
            "Epoch 179/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 68.2778\n",
            "Epoch 180/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 67.7804\n",
            "Epoch 181/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 67.2865\n",
            "Epoch 182/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 66.7963\n",
            "Epoch 183/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 66.3097\n",
            "Epoch 184/500\n",
            "1/1 [==============================] - 0s 147ms/step - loss: 65.8265\n",
            "Epoch 185/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 65.3470\n",
            "Epoch 186/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 64.8710\n",
            "Epoch 187/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 64.3983\n",
            "Epoch 188/500\n",
            "1/1 [==============================] - 0s 16ms/step - loss: 63.9291\n",
            "Epoch 189/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 63.4634\n",
            "Epoch 190/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 63.0009\n",
            "Epoch 191/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 62.5420\n",
            "Epoch 192/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 62.0863\n",
            "Epoch 193/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 61.6340\n",
            "Epoch 194/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 61.1849\n",
            "Epoch 195/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 60.7392\n",
            "Epoch 196/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 60.2967\n",
            "Epoch 197/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 59.8573\n",
            "Epoch 198/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 59.4213\n",
            "Epoch 199/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 58.9884\n",
            "Epoch 200/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 58.5585\n",
            "Epoch 201/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 58.1319\n",
            "Epoch 202/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 57.7084\n",
            "Epoch 203/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 57.2880\n",
            "Epoch 204/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 56.8706\n",
            "Epoch 205/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 56.4563\n",
            "Epoch 206/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 56.0449\n",
            "Epoch 207/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 55.6366\n",
            "Epoch 208/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 55.2313\n",
            "Epoch 209/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 54.8289\n",
            "Epoch 210/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 54.4293\n",
            "Epoch 211/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 54.0329\n",
            "Epoch 212/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 53.6392\n",
            "Epoch 213/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 53.2484\n",
            "Epoch 214/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 52.8604\n",
            "Epoch 215/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 52.4753\n",
            "Epoch 216/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 52.0930\n",
            "Epoch 217/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 51.7135\n",
            "Epoch 218/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 51.3367\n",
            "Epoch 219/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 50.9627\n",
            "Epoch 220/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 50.5914\n",
            "Epoch 221/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 50.2229\n",
            "Epoch 222/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 49.8569\n",
            "Epoch 223/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 49.4937\n",
            "Epoch 224/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 49.1331\n",
            "Epoch 225/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 48.7752\n",
            "Epoch 226/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 48.4198\n",
            "Epoch 227/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 48.0671\n",
            "Epoch 228/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 47.7169\n",
            "Epoch 229/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 47.3692\n",
            "Epoch 230/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 47.0241\n",
            "Epoch 231/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 46.6815\n",
            "Epoch 232/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 46.3414\n",
            "Epoch 233/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 46.0037\n",
            "Epoch 234/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 45.6686\n",
            "Epoch 235/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 45.3359\n",
            "Epoch 236/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 45.0056\n",
            "Epoch 237/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 44.6778\n",
            "Epoch 238/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 44.3522\n",
            "Epoch 239/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 44.0291\n",
            "Epoch 240/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 43.7083\n",
            "Epoch 241/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 43.3899\n",
            "Epoch 242/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 43.0737\n",
            "Epoch 243/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 42.7599\n",
            "Epoch 244/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 42.4484\n",
            "Epoch 245/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 42.1392\n",
            "Epoch 246/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 41.8322\n",
            "Epoch 247/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 41.5274\n",
            "Epoch 248/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 41.2249\n",
            "Epoch 249/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 40.9245\n",
            "Epoch 250/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 40.6263\n",
            "Epoch 251/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 40.3303\n",
            "Epoch 252/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 40.0365\n",
            "Epoch 253/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 39.7449\n",
            "Epoch 254/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 39.4552\n",
            "Epoch 255/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 39.1679\n",
            "Epoch 256/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 38.8824\n",
            "Epoch 257/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 38.5992\n",
            "Epoch 258/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 38.3180\n",
            "Epoch 259/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 38.0388\n",
            "Epoch 260/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 37.7617\n",
            "Epoch 261/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 37.4865\n",
            "Epoch 262/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 37.2134\n",
            "Epoch 263/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 36.9423\n",
            "Epoch 264/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.6731\n",
            "Epoch 265/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 36.4059\n",
            "Epoch 266/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 36.1407\n",
            "Epoch 267/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 35.8774\n",
            "Epoch 268/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.6161\n",
            "Epoch 269/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 35.3566\n",
            "Epoch 270/500\n",
            "1/1 [==============================] - 0s 17ms/step - loss: 35.0990\n",
            "Epoch 271/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 34.8433\n",
            "Epoch 272/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 34.5894\n",
            "Epoch 273/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 34.3374\n",
            "Epoch 274/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 34.0873\n",
            "Epoch 275/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 33.8389\n",
            "Epoch 276/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 33.5924\n",
            "Epoch 277/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 33.3476\n",
            "Epoch 278/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 33.1047\n",
            "Epoch 279/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 32.8635\n",
            "Epoch 280/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 32.6241\n",
            "Epoch 281/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 32.3864\n",
            "Epoch 282/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 32.1504\n",
            "Epoch 283/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 31.9162\n",
            "Epoch 284/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 31.6836\n",
            "Epoch 285/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 31.4528\n",
            "Epoch 286/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 31.2237\n",
            "Epoch 287/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 30.9962\n",
            "Epoch 288/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 30.7704\n",
            "Epoch 289/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 30.5461\n",
            "Epoch 290/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 30.3237\n",
            "Epoch 291/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 30.1027\n",
            "Epoch 292/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 29.8834\n",
            "Epoch 293/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 29.6657\n",
            "Epoch 294/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 29.4496\n",
            "Epoch 295/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 29.2350\n",
            "Epoch 296/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 29.0220\n",
            "Epoch 297/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.8106\n",
            "Epoch 298/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 28.6007\n",
            "Epoch 299/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 28.3923\n",
            "Epoch 300/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 28.1855\n",
            "Epoch 301/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 27.9801\n",
            "Epoch 302/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 27.7763\n",
            "Epoch 303/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 27.5739\n",
            "Epoch 304/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 27.3730\n",
            "Epoch 305/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 27.1736\n",
            "Epoch 306/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 26.9756\n",
            "Epoch 307/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 26.7791\n",
            "Epoch 308/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 26.5840\n",
            "Epoch 309/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 26.3903\n",
            "Epoch 310/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 26.1980\n",
            "Epoch 311/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 26.0072\n",
            "Epoch 312/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 25.8177\n",
            "Epoch 313/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 25.6296\n",
            "Epoch 314/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 25.4428\n",
            "Epoch 315/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 25.2575\n",
            "Epoch 316/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 25.0735\n",
            "Epoch 317/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 24.8908\n",
            "Epoch 318/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 24.7094\n",
            "Epoch 319/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 24.5294\n",
            "Epoch 320/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 24.3507\n",
            "Epoch 321/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 24.1733\n",
            "Epoch 322/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 23.9972\n",
            "Epoch 323/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 23.8224\n",
            "Epoch 324/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 23.6488\n",
            "Epoch 325/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 23.4765\n",
            "Epoch 326/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 23.3055\n",
            "Epoch 327/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 23.1357\n",
            "Epoch 328/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 22.9671\n",
            "Epoch 329/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 22.7998\n",
            "Epoch 330/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 22.6336\n",
            "Epoch 331/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 22.4688\n",
            "Epoch 332/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 22.3050\n",
            "Epoch 333/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 22.1426\n",
            "Epoch 334/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.9813\n",
            "Epoch 335/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 21.8211\n",
            "Epoch 336/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 21.6621\n",
            "Epoch 337/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.5043\n",
            "Epoch 338/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.3476\n",
            "Epoch 339/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 21.1921\n",
            "Epoch 340/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 21.0377\n",
            "Epoch 341/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 20.8845\n",
            "Epoch 342/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 20.7323\n",
            "Epoch 343/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 20.5812\n",
            "Epoch 344/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 20.4313\n",
            "Epoch 345/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 20.2824\n",
            "Epoch 346/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 20.1347\n",
            "Epoch 347/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 19.9880\n",
            "Epoch 348/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 19.8424\n",
            "Epoch 349/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 19.6978\n",
            "Epoch 350/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 19.5543\n",
            "Epoch 351/500\n",
            "1/1 [==============================] - 0s 185ms/step - loss: 19.4118\n",
            "Epoch 352/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 19.2704\n",
            "Epoch 353/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 19.1300\n",
            "Epoch 354/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 18.9906\n",
            "Epoch 355/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 18.8522\n",
            "Epoch 356/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 18.7149\n",
            "Epoch 357/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 18.5785\n",
            "Epoch 358/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 18.4432\n",
            "Epoch 359/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 18.3088\n",
            "Epoch 360/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 18.1754\n",
            "Epoch 361/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 18.0430\n",
            "Epoch 362/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 17.9115\n",
            "Epoch 363/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 17.7810\n",
            "Epoch 364/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.6515\n",
            "Epoch 365/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.5229\n",
            "Epoch 366/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 17.3952\n",
            "Epoch 367/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 17.2685\n",
            "Epoch 368/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 17.1427\n",
            "Epoch 369/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 17.0178\n",
            "Epoch 370/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 16.8938\n",
            "Epoch 371/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 16.7708\n",
            "Epoch 372/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 16.6486\n",
            "Epoch 373/500\n",
            "1/1 [==============================] - 0s 18ms/step - loss: 16.5273\n",
            "Epoch 374/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 16.4069\n",
            "Epoch 375/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 16.2873\n",
            "Epoch 376/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 16.1686\n",
            "Epoch 377/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 16.0508\n",
            "Epoch 378/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 15.9339\n",
            "Epoch 379/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 15.8178\n",
            "Epoch 380/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 15.7026\n",
            "Epoch 381/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 15.5882\n",
            "Epoch 382/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.4746\n",
            "Epoch 383/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 15.3618\n",
            "Epoch 384/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 15.2499\n",
            "Epoch 385/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 15.1388\n",
            "Epoch 386/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 15.0285\n",
            "Epoch 387/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.9191\n",
            "Epoch 388/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 14.8104\n",
            "Epoch 389/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 14.7024\n",
            "Epoch 390/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 14.5953\n",
            "Epoch 391/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 14.4890\n",
            "Epoch 392/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 14.3835\n",
            "Epoch 393/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.2787\n",
            "Epoch 394/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 14.1746\n",
            "Epoch 395/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 14.0714\n",
            "Epoch 396/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.9688\n",
            "Epoch 397/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.8671\n",
            "Epoch 398/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.7660\n",
            "Epoch 399/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.6658\n",
            "Epoch 400/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.5662\n",
            "Epoch 401/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.4674\n",
            "Epoch 402/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.3692\n",
            "Epoch 403/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 13.2718\n",
            "Epoch 404/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 13.1751\n",
            "Epoch 405/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 13.0791\n",
            "Epoch 406/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.9839\n",
            "Epoch 407/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.8892\n",
            "Epoch 408/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.7954\n",
            "Epoch 409/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.7021\n",
            "Epoch 410/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.6096\n",
            "Epoch 411/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 12.5177\n",
            "Epoch 412/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.4265\n",
            "Epoch 413/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.3360\n",
            "Epoch 414/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.2461\n",
            "Epoch 415/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 12.1569\n",
            "Epoch 416/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 12.0683\n",
            "Epoch 417/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.9804\n",
            "Epoch 418/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.8931\n",
            "Epoch 419/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.8065\n",
            "Epoch 420/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 11.7205\n",
            "Epoch 421/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 11.6351\n",
            "Epoch 422/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.5503\n",
            "Epoch 423/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.4662\n",
            "Epoch 424/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 11.3826\n",
            "Epoch 425/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.2997\n",
            "Epoch 426/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 11.2174\n",
            "Epoch 427/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.1357\n",
            "Epoch 428/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 11.0545\n",
            "Epoch 429/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.9740\n",
            "Epoch 430/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.8940\n",
            "Epoch 431/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.8147\n",
            "Epoch 432/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.7358\n",
            "Epoch 433/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.6576\n",
            "Epoch 434/500\n",
            "1/1 [==============================] - 0s 200ms/step - loss: 10.5800\n",
            "Epoch 435/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 10.5029\n",
            "Epoch 436/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.4264\n",
            "Epoch 437/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.3504\n",
            "Epoch 438/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 10.2750\n",
            "Epoch 439/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 10.2001\n",
            "Epoch 440/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.1259\n",
            "Epoch 441/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 10.0521\n",
            "Epoch 442/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.9788\n",
            "Epoch 443/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.9061\n",
            "Epoch 444/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 9.8339\n",
            "Epoch 445/500\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 9.7623\n",
            "Epoch 446/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.6912\n",
            "Epoch 447/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 9.6206\n",
            "Epoch 448/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 9.5505\n",
            "Epoch 449/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 9.4809\n",
            "Epoch 450/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.4118\n",
            "Epoch 451/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.3433\n",
            "Epoch 452/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.2752\n",
            "Epoch 453/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.2076\n",
            "Epoch 454/500\n",
            "1/1 [==============================] - 0s 14ms/step - loss: 9.1405\n",
            "Epoch 455/500\n",
            "1/1 [==============================] - 0s 11ms/step - loss: 9.0740\n",
            "Epoch 456/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 9.0078\n",
            "Epoch 457/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.9422\n",
            "Epoch 458/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.8770\n",
            "Epoch 459/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 8.8124\n",
            "Epoch 460/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.7482\n",
            "Epoch 461/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.6844\n",
            "Epoch 462/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.6212\n",
            "Epoch 463/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.5584\n",
            "Epoch 464/500\n",
            "1/1 [==============================] - 0s 13ms/step - loss: 8.4960\n",
            "Epoch 465/500\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 8.4341\n",
            "Epoch 466/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.3727\n",
            "Epoch 467/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.3117\n",
            "Epoch 468/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.2511\n",
            "Epoch 469/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 8.1910\n",
            "Epoch 470/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.1313\n",
            "Epoch 471/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0721\n",
            "Epoch 472/500\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 8.0132\n",
            "Epoch 473/500\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 7.9549\n",
            "Epoch 474/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.8969\n",
            "Epoch 475/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.8394\n",
            "Epoch 476/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.7823\n",
            "Epoch 477/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.7256\n",
            "Epoch 478/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.6693\n",
            "Epoch 479/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.6134\n",
            "Epoch 480/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.5580\n",
            "Epoch 481/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.5029\n",
            "Epoch 482/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.4482\n",
            "Epoch 483/500\n",
            "1/1 [==============================] - 0s 8ms/step - loss: 7.3939\n",
            "Epoch 484/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.3401\n",
            "Epoch 485/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2866\n",
            "Epoch 486/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 7.2335\n",
            "Epoch 487/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1808\n",
            "Epoch 488/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 7.1285\n",
            "Epoch 489/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.0766\n",
            "Epoch 490/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 7.0250\n",
            "Epoch 491/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.9738\n",
            "Epoch 492/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.9230\n",
            "Epoch 493/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.8726\n",
            "Epoch 494/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.8225\n",
            "Epoch 495/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.7728\n",
            "Epoch 496/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.7234\n",
            "Epoch 497/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.6745\n",
            "Epoch 498/500\n",
            "1/1 [==============================] - 0s 7ms/step - loss: 6.6259\n",
            "Epoch 499/500\n",
            "1/1 [==============================] - 0s 5ms/step - loss: 6.5776\n",
            "Epoch 500/500\n",
            "1/1 [==============================] - 0s 6ms/step - loss: 6.5296\n",
            "[403.68585]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 4 Excerise"
      ],
      "metadata": {
        "id": "HIKpPJE8cZQj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Load fashion dataset\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZDBjcahdAa7",
        "outputId": "9070726a-e43f-44d2-b2bf-eb72d8831958"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
            "32768/29515 [=================================] - 0s 0us/step\n",
            "40960/29515 [=========================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
            "26427392/26421880 [==============================] - 0s 0us/step\n",
            "26435584/26421880 [==============================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
            "16384/5148 [===============================================================================================] - 0s 0us/step\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
            "4423680/4422102 [==============================] - 0s 0us/step\n",
            "4431872/4422102 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#view data\n",
        "plt.imshow(training_images[0])\n",
        "print(training_labels[0])\n",
        "print(training_images[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 800
        },
        "id": "7SvhecN2gLPs",
        "outputId": "2c5da971-d181-469b-c048-525016f15fd8"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "9\n",
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   1   0   0  13  73   0   0   1   4   0   0   0   0   1   1   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   3   0  36 136 127  62  54   0   0   0   1   3   4   0   0   3]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   6   0 102 204 176 134 144 123  23   0   0   0   0  12  10   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 155 236 207 178 107 156 161 109  64  23  77 130  72  15]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   1   0  69 207 223 218 216 216 163 127 121 122 146 141  88 172  66]\n",
            " [  0   0   0   0   0   0   0   0   0   1   1   1   0 200 232 232 233 229 223 223 215 213 164 127 123 196 229   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 183 225 216 223 228 235 227 224 222 224 221 223 245 173   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0 193 228 218 213 198 180 212 210 211 213 223 220 243 202   0]\n",
            " [  0   0   0   0   0   0   0   0   0   1   3   0  12 219 220 212 218 192 169 227 208 218 224 212 226 197 209  52]\n",
            " [  0   0   0   0   0   0   0   0   0   0   6   0  99 244 222 220 218 203 198 221 215 213 222 220 245 119 167  56]\n",
            " [  0   0   0   0   0   0   0   0   0   4   0   0  55 236 228 230 228 240 232 213 218 223 234 217 217 209  92   0]\n",
            " [  0   0   1   4   6   7   2   0   0   0   0   0 237 226 217 223 222 219 222 221 216 223 229 215 218 255  77   0]\n",
            " [  0   3   0   0   0   0   0   0   0  62 145 204 228 207 213 221 218 208 211 218 224 223 219 215 224 244 159   0]\n",
            " [  0   0   0   0  18  44  82 107 189 228 220 222 217 226 200 205 211 230 224 234 176 188 250 248 233 238 215   0]\n",
            " [  0  57 187 208 224 221 224 208 204 214 208 209 200 159 245 193 206 223 255 255 221 234 221 211 220 232 246   0]\n",
            " [  3 202 228 224 221 211 211 214 205 205 205 220 240  80 150 255 229 221 188 154 191 210 204 209 222 228 225   0]\n",
            " [ 98 233 198 210 222 229 229 234 249 220 194 215 217 241  65  73 106 117 168 219 221 215 217 223 223 224 229  29]\n",
            " [ 75 204 212 204 193 205 211 225 216 185 197 206 198 213 240 195 227 245 239 223 218 212 209 222 220 221 230  67]\n",
            " [ 48 203 183 194 213 197 185 190 194 192 202 214 219 221 220 236 225 216 199 206 186 181 177 172 181 205 206 115]\n",
            " [  0 122 219 193 179 171 183 196 204 210 213 207 211 210 200 196 194 191 195 191 198 192 176 156 167 177 210  92]\n",
            " [  0   0  74 189 212 191 175 172 175 181 185 188 189 188 193 198 204 209 210 210 211 188 188 194 192 216 170   0]\n",
            " [  2   0   0   0  66 200 222 237 239 242 246 243 244 221 220 193 191 179 182 182 181 176 166 168  99  58   0   0]\n",
            " [  0   0   0   0   0   0   0  40  61  44  72  41  35   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Normalize data \n",
        "training_images = training_images/255\n",
        "test_images = test_images/255"
      ],
      "metadata": {
        "id": "-_ibD6PcgyEv"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib.cbook import flatten\n",
        "#Design model\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                                   tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "                                   tf.keras.layers.Dense(10, activation=\"softmax\")])"
      ],
      "metadata": {
        "id": "xLdSHsRkg95m"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=\"adam\",\n",
        "              loss= \"sparse_categorical_crossentropy\",\n",
        "              metrics = [\"accuracy\"])\n",
        "model.fit(training_images, training_labels, epochs=5)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWUODQIoifEG",
        "outputId": "a0a0d641-d47a-40b4-b7cb-7847218d4ccf"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 7s 4ms/step - loss: 0.4993 - accuracy: 0.8247\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3774 - accuracy: 0.8653\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3373 - accuracy: 0.8768\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.3122 - accuracy: 0.8845\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 5s 3ms/step - loss: 0.2957 - accuracy: 0.8906\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46d3b49710>"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_images, test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0UV159Qk1c5",
        "outputId": "f5cd0ea5-2669-458b-c208-f2e2630db6c6"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.3549 - accuracy: 0.8730\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3548794090747833, 0.8730000257492065]"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "classifications = model.predict(test_images)\n",
        "print(tf.math.round(classifications[:5]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-mxud3VlZzR",
        "outputId": "9f540264-e7db-4d8b-81f0-1524713cedb2"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(5, 10), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction [9,2,1,1,6]\n",
        "test_labels[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lpJguULpnu1R",
        "outputId": "488d452a-2c13-4016-8cc2-472e8259d2bb"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([9, 2, 1, 1, 6], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Outmate classification\n",
        "output = tf.gather(tf.math.round(classifications[:5]), 0)\n",
        "output"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOZse-cRrbAH",
        "outputId": "22910bfe-fbbc-4fc9-8fe2-7fda465b477b"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_nine = tf.constant([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.])\n",
        "class_nine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zv-Svdsmu7eB",
        "outputId": "a57a3837-522a-43b3-9df1-343dadc8c01a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=float32, numpy=array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def classification_automation(output):\n",
        "#   if output ==  np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 1.], dtype=float32):\n",
        "#     print(\"class 09\") \n",
        "# => If statements do not work for tensorflow"
      ],
      "metadata": {
        "id": "r8twKsRYr3rw"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "a = tf.math.equal(class_nine, output)\n",
        "a"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7thBrGKx0dr",
        "outputId": "5f5f9f78-371a-4ed9-ecdd-9de5baea75fe"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True,  True,  True])>"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class_eight = tf.constant([0., 0., 0., 0., 0., 0., 0., 0., 1., 0.])\n",
        "b = tf.math.equal(class_eight, output)\n",
        "b"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6cnWuuRi-W1v",
        "outputId": "220084b4-8448-4c44-b8ac-391352861c8d"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(10,), dtype=bool, numpy=array([ True,  True,  True,  True,  True,  True,  True,  True, False, False])>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(a.numpy()) #=> correct class\n",
        "print(b.numpy()) #=> wrong class"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r-vyAxdSAsR6",
        "outputId": "3d591dcc-7c9a-4553-9b2c-4fe157714c2e"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[ True  True  True  True  True  True  True  True  True  True]\n",
            "[ True  True  True  True  True  True  True  True False False]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5 Ex"
      ],
      "metadata": {
        "id": "xEdJlqsKCBKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Now test with 512 neurons\n",
        "#What different results do you get for loss, training time etc? \n",
        "#Why do you think that's the case?\n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "viOkXZfpCEB-",
        "outputId": "e70c9cc9-024c-45a2-accf-2c390ddab8e5"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4771 - accuracy: 0.8302\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3599 - accuracy: 0.8686\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3221 - accuracy: 0.8816\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2984 - accuracy: 0.8897\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2814 - accuracy: 0.8962\n",
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3381 - accuracy: 0.8781\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xy8a_RuMH1ES",
        "outputId": "7ff2e03f-6e65-45d4-a159-10f0a0f5fece"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Hypothesis: Double the amount of neurons will increase accuracy\n",
        "#But processing will take more time \n",
        "\n",
        "model = tf.keras.models.Sequential([tf.keras.layers.Flatten(),\n",
        "                          tf.keras.layers.Dense(1024, activation=\"relu\"),\n",
        "                          tf.keras.layers.Dense(10, activation=\"softmax\")])\n",
        "\n",
        "model.compile(optimizer=\"adam\", \n",
        "              loss=\"sparse_categorical_crossentropy\",\n",
        "              metrics=[\"accuracy\"])\n",
        "\n",
        "model.fit(training_images, training_labels, epochs=5)\n",
        "\n",
        "model.evaluate(test_images, test_labels)\n",
        "\n",
        "classification = model.predict(test_images)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GdwifcFcINAC",
        "outputId": "0b7a94b6-8a69-4bf2-d5e7-6225544436bf"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.4712 - accuracy: 0.8321\n",
            "Epoch 2/5\n",
            "1875/1875 [==============================] - 16s 9ms/step - loss: 0.3583 - accuracy: 0.8682\n",
            "Epoch 3/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.3226 - accuracy: 0.8813\n",
            "Epoch 4/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2969 - accuracy: 0.8901\n",
            "Epoch 5/5\n",
            "1875/1875 [==============================] - 17s 9ms/step - loss: 0.2816 - accuracy: 0.8956\n",
            "313/313 [==============================] - 2s 7ms/step - loss: 0.3326 - accuracy: 0.8821\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Result: Training takes longer and accuracy is increased from 0.8656 to 0.8757\n",
        "print(tf.math.round(classifications[0],0))\n",
        "print(test_labels[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NrfGX58nJXaF",
        "outputId": "88622ff7-62fc-4a54-c772-57693916a295"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor([0. 0. 0. 0. 0. 0. 0. 0. 0. 1.], shape=(10,), dtype=float32)\n",
            "9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6 Ex"
      ],
      "metadata": {
        "id": "5TM3gP1YCKlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#What would happen if you remove the Flatten() layer. Why do you think that's the case?\n",
        "\n",
        "#=> Error: Because shape of data. \n",
        "#RoT = First Layer should be in same shape as input data \n",
        "#With Flatten we create of out 28x28 matrix an array of 784X1\n"
      ],
      "metadata": {
        "id": "xLCWk4JyCPCc"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7 Ex"
      ],
      "metadata": {
        "id": "nESok22sCONn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the final (output) layers. Why are there 10 of them? \n",
        "#What would happen if you had a different amount than 10? For example, try training the network with 5.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "PJb3ydI7CTPa"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8 Ex"
      ],
      "metadata": {
        "id": "0ddP6xKNCUDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the effects of additional layers in the network. \n",
        "#What will happen if you add another layer between the one with 512 and the final layer with 10.\n",
        "\n",
        "#=>"
      ],
      "metadata": {
        "id": "_8gw2Jp9CbDV"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 9 Ex"
      ],
      "metadata": {
        "id": "tJ3meIPJCYyZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Consider the impact of training for more or less epochs. Why do you think that would be the case?\n",
        "\n",
        "#=>\n"
      ],
      "metadata": {
        "id": "BDs6n82hCYT6"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 10 Ex"
      ],
      "metadata": {
        "id": "QCWoo1T5XTd6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Before you trained, you normalized the data, going from values that were 0-255 to values that were 0-1. \n",
        "#What would be the impact of removing that? Here's the complete code to give it a try. Why do you think you get different results?\n",
        "\n",
        "#=>\n",
        "\n"
      ],
      "metadata": {
        "id": "5sOEqevvXWeL"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 11 Ex"
      ],
      "metadata": {
        "id": "Xpa-faKNXjIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Earlier when you trained for extra epochs you had an issue where your loss might change. It might have taken a bit of time for you to wait for the training to do that, and you might have thought 'wouldn't it be nice if I could stop the training when I reach a desired value?' -- i.e. 95% accuracy might be enough for you, and if you reach that after 3 epochs, why sit around waiting for it to finish a lot more epochs....So how would you fix that? Like any other program...you have callbacks! Let's see them in action..."
      ],
      "metadata": {
        "id": "o1cHN4d1XwRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class myCallback(tf.keras.callbacks.Callback):\n",
        "  def on_epoch_end(self, epoch, logs={}):\n",
        "    if(logs.get(\"acc\") > 0.9):\n",
        "      print(\"\\nReached 85% of accuracy so cancelling training!\")\n",
        "      self.model.stop_training = True\n",
        "  \n",
        "callbacks = myCallback()\n",
        "\n",
        "#Create Dataset\n",
        "mnist = tf.keras.datasets.fashion_mnist\n",
        "(training_images, training_labels), (test_images, test_labels) = mnist.load_data()\n",
        "training_images = training_images/255\n",
        "test_images = test_images/255\n",
        "\n",
        "#Create model\n",
        "model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation=\"relu\"),\n",
        "    tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=\"acc\")\n",
        "model.fit(training_images, training_labels, epochs=8, callbacks=[callbacks])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SFuOEyMbxhTs",
        "outputId": "be072141-9ac6-49ef-ab0b-6b1653e03294"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.4723 - acc: 0.8309\n",
            "Epoch 2/8\n",
            "1875/1875 [==============================] - 11s 6ms/step - loss: 0.3558 - acc: 0.8694\n",
            "Epoch 3/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.3221 - acc: 0.8808\n",
            "Epoch 4/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2991 - acc: 0.8889\n",
            "Epoch 5/8\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2798 - acc: 0.8952\n",
            "Epoch 6/8\n",
            "1873/1875 [============================>.] - ETA: 0s - loss: 0.2652 - acc: 0.9011\n",
            "Reached 85% of accuracy so cancelling training!\n",
            "1875/1875 [==============================] - 10s 5ms/step - loss: 0.2651 - acc: 0.9012\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f46c6403750>"
            ]
          },
          "metadata": {},
          "execution_count": 68
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Performance evaluation\n",
        "model.evaluate(test_images, test_labels)\n",
        "predictions = model.predict(test_images)\n",
        "print(tf.math.round(predictions[:5]))\n",
        "print(test_labels[:5])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Plem09ri5jn1",
        "outputId": "12f10e12-9a22-4d58-86bb-b8a8f41b12e8"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "313/313 [==============================] - 1s 3ms/step - loss: 0.3349 - acc: 0.8810\n",
            "tf.Tensor(\n",
            "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]], shape=(5, 10), dtype=float32)\n",
            "[9 2 1 1 6]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Day 2: TF.ComputerVision (CV)"
      ],
      "metadata": {
        "id": "j3GUyO_Df9l3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "sKHDSeb8gBam"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 3: TF.Natrual Language Processing (NLP)"
      ],
      "metadata": {
        "id": "YsyqqzzVgCdX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "yGrzdqAjgIYC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 4: TF.Time Series Forcasting"
      ],
      "metadata": {
        "id": "1AnDmEu3gMew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "MKNNViULgUSA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}